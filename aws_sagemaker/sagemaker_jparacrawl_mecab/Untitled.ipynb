{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b8baaa01-181c-47a4-ae59-66b90ee81fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/tetsuya/repo/programming-study/aws_sagemaker/sagemaker_jparacrawl_mecab/data\n"
     ]
    }
   ],
   "source": [
    "%cd data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf70d9d7-e4b6-4f11-ab37-e07df58c7938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-11-25 18:41:54--  https://www.kecl.ntt.co.jp/icl/lirg/jparacrawl/release/3.0/bitext/en-ja.tar.gz\n",
      "www.kecl.ntt.co.jp (www.kecl.ntt.co.jp) をDNSに問いあわせています... 163.137.218.58\n",
      "www.kecl.ntt.co.jp (www.kecl.ntt.co.jp)|163.137.218.58|:443 に接続しています... 接続しました。\n",
      "HTTP による接続要求を送信しました、応答を待っています... 200 OK\n",
      "長さ: 2972770796 (2.8G) [application/x-gzip]\n",
      "`en-ja.tar.gz' に保存中\n",
      "\n",
      "en-ja.tar.gz        100%[===================>]   2.77G  4.06MB/s    in 10m 52s \n",
      "\n",
      "2022-11-25 18:52:46 (4.35 MB/s) - `en-ja.tar.gz' へ保存完了 [2972770796/2972770796]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://www.kecl.ntt.co.jp/icl/lirg/jparacrawl/release/3.0/bitext/en-ja.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e713a5db-3ffe-4d70-901b-7416240643fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar xvf en-ja.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d47c3453-6060-46f1-a1b1-191a6fcbb2a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/tetsuya/repo/programming-study/aws_sagemaker/sagemaker_jparacrawl_mecab/data/en-ja\n"
     ]
    }
   ],
   "source": [
    "%cd en-ja/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16fd33d2-c4ef-403b-b3bd-74215b589626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64394\n"
     ]
    }
   ],
   "source": [
    "!cut -f 1 en-ja.bicleaner05.txt | uniq | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05de12e9-d446-4e8a-88bc-c072b40faf30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0001vip.cocolog-nifty.com\n",
      "000-lhr.web.wox.cc\n",
      "0017.org\n",
      "001sprint.com\n",
      "0024.review.wox.cc\n",
      "011300.web.wox.cc\n",
      "01e-coach.roundtable.jp\n",
      "0257.jp\n",
      "0327.r-fs11136.net\n",
      "03-willcom.windows-keitai.com\n",
      "uniq: 書き込みエラー: Broken pipe\n"
     ]
    }
   ],
   "source": [
    "!cut -f 1 en-ja.bicleaner05.txt | uniq | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de95b146-8067-4bed-af02-1328a4fba53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat en-ja.bicleaner05.txt | cut -f4 > en.txt\n",
    "!cat en-ja.bicleaner05.txt | cut -f5 > ja.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f0e33d7-c0de-454c-9456-1f1b1c1809ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mecab-python3\n",
      "  Downloading mecab_python3-1.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (581 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m581.1/581.1 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: mecab-python3\n",
      "Successfully installed mecab-python3-1.0.5\n",
      "Collecting unidic\n",
      "  Downloading unidic-1.1.0.tar.gz (7.7 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.22.0 in /home/tetsuya/anaconda3/envs/sagemaker/lib/python3.10/site-packages (from unidic) (2.28.1)\n",
      "Collecting tqdm<5.0.0,>=4.41.1\n",
      "  Using cached tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
      "Collecting wasabi<1.0.0,>=0.6.0\n",
      "  Downloading wasabi-0.10.1-py3-none-any.whl (26 kB)\n",
      "Collecting plac<2.0.0,>=1.1.3\n",
      "  Downloading plac-1.3.5-py2.py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/tetsuya/anaconda3/envs/sagemaker/lib/python3.10/site-packages (from requests<3.0.0,>=2.22.0->unidic) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/tetsuya/anaconda3/envs/sagemaker/lib/python3.10/site-packages (from requests<3.0.0,>=2.22.0->unidic) (2022.9.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/tetsuya/anaconda3/envs/sagemaker/lib/python3.10/site-packages (from requests<3.0.0,>=2.22.0->unidic) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/tetsuya/anaconda3/envs/sagemaker/lib/python3.10/site-packages (from requests<3.0.0,>=2.22.0->unidic) (3.4)\n",
      "Building wheels for collected packages: unidic\n",
      "  Building wheel for unidic (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for unidic: filename=unidic-1.1.0-py3-none-any.whl size=7409 sha256=519dada52465ae848c867a2191c2110c2c0d468124b0d89264470844b258a3e0\n",
      "  Stored in directory: /home/tetsuya/.cache/pip/wheels/7a/72/72/1f3d654c345ea69d5d51b531c90daf7ba14cc555eaf2c64ab0\n",
      "Successfully built unidic\n",
      "Installing collected packages: wasabi, plac, tqdm, unidic\n",
      "Successfully installed plac-1.3.5 tqdm-4.64.1 unidic-1.1.0 wasabi-0.10.1\n"
     ]
    }
   ],
   "source": [
    "!pip install mecab-python3\n",
    "!pip install unidic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6748378-3cdc-4b67-a93e-4bd6415541c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download url: https://cotonoha-dic.s3-ap-northeast-1.amazonaws.com/unidic-3.1.0.zip\n",
      "Dictionary version: 3.1.0+2021-08-31\n",
      "Downloading UniDic v3.1.0+2021-08-31...\n",
      "unidic-3.1.0.zip: 100%|██████████████████████| 526M/526M [01:00<00:00, 8.73MB/s]\n",
      "Finished download.\n",
      "Downloaded UniDic v3.1.0+2021-08-31 to /home/tetsuya/anaconda3/envs/sagemaker/lib/python3.10/site-packages/unidic/dicdir\n"
     ]
    }
   ],
   "source": [
    "!python -m unidic download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2df0ce14-296b-4f03-868c-7c8b811de207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "くる\t動詞,非自立可能,,,カ行変格,連体形-一般,クル,来る,くる,クル,くる,クル,和,\"\",\"\",\"\",\"\",\"\",\"\",用,クル,クル,クル,クル,\"1\",\"C1\",\"\",2891174448865985,10518\n",
      "まで\t助詞,副助詞,,,,,マデ,まで,まで,マデ,まで,マデ,和,\"\",\"\",\"\",\"\",\"\",\"\",副助,マデ,マデ,マデ,マデ,\"\",\"名詞%F2@1,形容詞%F2@1,動詞%F2@1\",\"\",9865651581755904,35891\n",
      "まつ\t動詞,一般,,,五段-タ行,連体形-一般,マツ,待つ,まつ,マツ,まつ,マツ,和,\"\",\"\",\"\",\"\",\"\",\"\",用,マツ,マツ,マツ,マツ,\"1\",\"C1\",\"\",9848884029432513,35830\n",
      "EOS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import MeCab\n",
    "import unidic\n",
    "\n",
    "tagger = MeCab.Tagger()  # 「tagger = MeCab.Tagger('-d ' + unidic.DICDIR)」\n",
    "sample_txt = 'くるまでまつ'\n",
    "result = tagger.parse(sample_txt)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9deb1b09-a366-4c52-aa1b-f8ba6bcc9b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "くるまでまつ\n",
      "くる まで まつ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "wakati = MeCab.Tagger('-Owakati')\n",
    "result = wakati.parse(sample_txt)\n",
    "print(sample_txt)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6556fd93-4449-4a0c-94ad-816c6d40f2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./ja.txt\") as in_f:\n",
    "    with open(\"ja_tokenized.txt\", mode='w') as out_f:\n",
    "        for line in in_f:\n",
    "            out_f.write(wakati.parse(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0a6c281-070c-4bed-bce9-1f2570d5f872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /home/tetsuya/anaconda3/envs/sagemaker/lib/python3.10/site-packages (0.1.97)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d2355e5-156b-4567-8189-a651081d50f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!shuf -n 100000 en.txt -o vocab_train.en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7aec3ee-f918-4066-b29e-97253b3a9d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=vocab_train.en --model_prefix=sentencepiece_en --vocab_size=32000 --character_coverage=0.98\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: vocab_train.en\n",
      "  input_format: \n",
      "  model_prefix: sentencepiece_en\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 32000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.98\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(350) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(181) LOG(INFO) Loading corpus: vocab_train.en\n",
      "trainer_interface.cc(406) LOG(INFO) Loaded all 100000 sentences\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(427) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(536) LOG(INFO) all chars count=14294781\n",
      "trainer_interface.cc(547) LOG(INFO) Done: 98.0288% characters are covered.\n",
      "trainer_interface.cc(557) LOG(INFO) Alphabet size=46\n",
      "trainer_interface.cc(558) LOG(INFO) Final character coverage=0.980288\n",
      "trainer_interface.cc(590) LOG(INFO) Done! preprocessed 100000 sentences.\n",
      "unigram_model_trainer.cc(146) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(150) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(201) LOG(INFO) Initialized 169075 seed sentencepieces\n",
      "trainer_interface.cc(596) LOG(INFO) Tokenizing input sentences with whitespace: 100000\n",
      "trainer_interface.cc(607) LOG(INFO) Done! 157768\n",
      "unigram_model_trainer.cc(491) LOG(INFO) Using 157768 sentences for EM training\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=67982 obj=14.5581 num_tokens=435036 num_tokens/piece=6.39928\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=59111 obj=12.3085 num_tokens=436525 num_tokens/piece=7.38484\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=44328 obj=12.2975 num_tokens=453824 num_tokens/piece=10.2379\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=44308 obj=12.277 num_tokens=454318 num_tokens/piece=10.2536\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=35197 obj=12.2712 num_tokens=472967 num_tokens/piece=13.4377\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=35195 obj=12.2935 num_tokens=473143 num_tokens/piece=13.4435\n",
      "trainer_interface.cc(685) LOG(INFO) Saving model: sentencepiece_en.model\n",
      "trainer_interface.cc(697) LOG(INFO) Saving vocabs: sentencepiece_en.vocab\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "spm.SentencePieceTrainer.Train(\n",
    "   '--input=vocab_train.en --model_prefix=sentencepiece_en --vocab_size=32000 --character_coverage=0.98'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70b0f77-abd7-4ab7-af59-c0ac799c650e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_en = spm.SentencePieceProcessor(\"sentencepiece_en.model\")\n",
    "\n",
    "with open(\"./en.txt\") as in_f:\n",
    "    with open(\"en_tokenized.txt\", mode='w') as out_f:\n",
    "        for line in in_f:\n",
    "            out_f.write(' '.join(sp_en.encode(line, out_type=str)) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ca7f51f-88a0-40bc-ab18-a99efbfcd88a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▁And ▁everyone ▁will ▁not ▁care ▁that ▁it ▁is ▁not ▁you .\n",
      "▁And ▁everyone ▁will ▁not ▁care ▁that ▁it ▁is ▁not ▁you .\n",
      "▁Sponsor ed ▁link ▁This ▁advertisement ▁is ▁displayed ▁when ▁there ▁is ▁no ▁update ▁for ▁a ▁certain ▁period ▁of ▁time .\n",
      "▁Also , ▁it ▁will ▁always ▁be ▁hidden ▁when ▁becoming ▁a ▁premium ▁user ▁ .\n",
      "▁It ▁will ▁return ▁to ▁non - display ▁when ▁content ▁update ▁is ▁done .\n",
      "▁Sponsor ed ▁link ▁This ▁advertisement ▁is ▁displayed ▁when ▁there ▁is ▁no ▁update ▁for ▁a ▁certain ▁period ▁of ▁time .\n",
      "▁Also , ▁it ▁will ▁always ▁be ▁hidden ▁when ▁becoming ▁a ▁premium ▁user ▁ .\n",
      "▁It ▁will ▁return ▁to ▁non - display ▁when ▁content ▁update ▁is ▁done .\n",
      "▁It ’ s ▁like ▁you ▁can ▁enrich ▁it ▁and ▁save ▁money ▁as ▁a ▁result . ▁I ’ ve ▁watched ▁a ▁lot ▁of ▁videos , ▁mainly ▁on ▁ Y outube , ▁of ▁people ▁who ▁say ▁they ’ re ▁minimalist s , ▁and ▁I ▁agree .\n",
      "▁ G o ▁to ▁the ▁original ▁video ▁hierarchy ▁of ▁the ▁conversion ▁source , ▁copy ▁and ▁paste ▁the ▁following ▁is ▁fine . ▁ ff m peg ▁ - i ▁sample . mp 4 ▁ - stric t ▁ -2 ▁video . web m ▁summary ▁I ’ ve ▁been ▁using ▁the ▁upload ▁and ▁embed ▁method ▁to ▁ Y outube ▁to ▁set ▁up ▁videos ▁on ▁the ▁web .\n"
     ]
    }
   ],
   "source": [
    "!head en_tokenized.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2fbeeb3d-a55b-4fc3-a48a-0ffd73e30931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "鼻 ・ 口 の ところ は あらかじめ 少し 切っ て おく と いい です ね 。 \n",
      "アドレス 置い とく の で 、 消さ れ ない うち に メール くれ たら 嬉しい です 。 \n",
      "スポンサー ドリンク この 広告 は 一定 期間 更新 が ない 場合 に 表示 さ れ ます 。 \n",
      "また 、 プレミアム ユーザー に なる と 常 に 非 表示 に なり ます 。 \n",
      "コンテンツ の 更新 が 行わ れる と 非 表示 に 戻り ます 。 \n",
      "スポンサー ドリンク この 広告 は 一定 期間 更新 が ない 場合 に 表示 さ れ ます 。 \n",
      "また 、 プレミアム ユーザー に なる と 常 に 非 表示 に なり ます 。 \n",
      "コンテンツ の 更新 が 行わ れる と 非 表示 に 戻り ます 。 \n",
      "You tube を 中心 に ミニマリスト と 言っ て いる 方 の 動画 を たくさん み まし た が 、 納得 いく もの の も 多く 、 不況 と 少子 化 を 呼ば れる ” 今 の ” 日本 に は 合っ て いる 考え 方 な の か な 、 と 感じ て い ます 。 \n",
      "ffmpeg - i sample . mp 4 - strict - 2 video . webm まとめ Web 上 で 動画 を 設置 する とき は 、 You tube に アップ し て 埋め込む 方法 ばかり 使っ て い まし た が 、 これ で 複数 の 動画 形式 を 作る こと が できる の で 、 自分 の サーバ に 設定 する こと も 可能 に なり ます ね 。 \n"
     ]
    }
   ],
   "source": [
    "!head ja_tokenized.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "370ba68c-4a02-44bb-a07c-1cd6919d7725",
   "metadata": {},
   "outputs": [],
   "source": [
    "!seq `wc -l en-ja.bicleaner05.txt | cut -f1 -d' '` | shuf -o line_nums.txt -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a92cf03b-5918-47f7-aef9-b4be3cd1aa02",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head --lines=25730000 line_nums.txt | sort --numeric-sort > train_line_nums.txt\n",
    "!head --lines=25735000 line_nums.txt | tail --lines=5000 | sort --numeric-sort > val_line_nums.txt\n",
    "!head --lines=25740000 line_nums.txt | tail --lines=5000 | sort --numeric-sort > test_line_nums.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b69de2ef-be7a-4915-a69b-a59cb7cef08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_file: 取り出し元ファイル\n",
    "# num_fileの行番号の行だけ取り出します。\n",
    "# output_file: 取り出した結果ファイル\n",
    "def extract_lines(input_file, num_file, output_file):\n",
    "    text_line_num = 1\n",
    "    with open(num_file) as line_f:\n",
    "        line_num = int(line_f.readline())\n",
    "        with open(input_file) as in_f:\n",
    "            with open(output_file, mode='w') as out_f:\n",
    "                for line in in_f:\n",
    "                    if text_line_num == line_num:\n",
    "                        out_f.write(line)\n",
    "                        line_num = line_f.readline()\n",
    "                        if line_num == '':\n",
    "                            break\n",
    "                        else:\n",
    "                            line_num = int(line_num)\n",
    "                    text_line_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7371dc00-abc2-44e3-a794-37b8982f66f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_lines(\"en_tokenized.txt\", \"train_line_nums.txt\", \"train.en\")\n",
    "extract_lines(\"en_tokenized.txt\", \"val_line_nums.txt\", \"val.en\")\n",
    "extract_lines(\"en_tokenized.txt\", \"test_line_nums.txt\", \"test.en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2434fee0-0d69-408e-8c42-0d397c48f950",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_lines(\"ja_tokenized.txt\", \"train_line_nums.txt\", \"train.ja\")\n",
    "extract_lines(\"ja_tokenized.txt\", \"val_line_nums.txt\", \"val.ja\")\n",
    "extract_lines(\"ja_tokenized.txt\", \"test_line_nums.txt\", \"test.ja\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f2342a32-e4ba-4869-afe3-7fdb2df3ad70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/tetsuya/repo/programming-study/aws_sagemaker/sagemaker_jparacrawl_mecab\n"
     ]
    }
   ],
   "source": [
    "%cd ../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9d48f662-92b4-472d-af9a-1adcd305e96a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus corpus_1's weight should be given. We default it to 1 for you.\n",
      "[2022-11-25 21:37:56,413 INFO] Counter vocab from 10000 samples.\n",
      "[2022-11-25 21:37:56,413 INFO] Build vocab on 10000 transformed examples/corpus.\n",
      "[2022-11-25 21:37:56,419 INFO] corpus_1's transforms: TransformPipe()\n",
      "[2022-11-25 21:37:56,694 INFO] Counters src:10532\n",
      "[2022-11-25 21:37:56,694 INFO] Counters tgt:11431\n"
     ]
    }
   ],
   "source": [
    "!onmt_build_vocab -config vocab_en_ja.yml -n_sample 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f8bcdcc0-4aa6-4695-9d84-1f649836dd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p data_train/en-ja\n",
    "!cp data/en-ja/jparacrawl.vocab.* data_train/en-ja\n",
    "!cp data/en-ja/train.* data_train/en-ja\n",
    "!cp data/en-ja/val.* data_train/en-ja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72fb0e4e-9363-4b93-879c-ebaf43c25326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! Your password will be stored unencrypted in /home/tetsuya/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "Sending build context to Docker daemon  1.171MB\n",
      "Step 1/9 : FROM nvidia/cuda:11.7.1-runtime-ubuntu20.04\n",
      " ---> 9a178fee7c22\n",
      "Step 2/9 : RUN apt-get -y update\n",
      " ---> Using cache\n",
      " ---> 4d7452ab4cd0\n",
      "Step 3/9 : RUN apt-get -y install python3 pip\n",
      " ---> Using cache\n",
      " ---> 8ed2c75c4f94\n",
      "Step 4/9 : RUN pip --no-cache-dir install OpenNMT-py\n",
      " ---> Using cache\n",
      " ---> ccfcabb82d71\n",
      "Step 5/9 : ENV PYTHONUNBUFFERED=TRUE\n",
      " ---> Using cache\n",
      " ---> b39c6aad63a5\n",
      "Step 6/9 : ENV PYTHONDONTWRITEBYTECODE=TRUE\n",
      " ---> Using cache\n",
      " ---> c8cb8c7e5898\n",
      "Step 7/9 : ENV PATH=\"/opt/program:${PATH}\"\n",
      " ---> Using cache\n",
      " ---> d43f14365ed3\n",
      "Step 8/9 : COPY src /opt/program\n",
      " ---> f1cb1c16fcd6\n",
      "Step 9/9 : WORKDIR /opt/program\n",
      " ---> Running in ff9abe4d0705\n",
      "Removing intermediate container ff9abe4d0705\n",
      " ---> 2ee8069a1ebb\n",
      "Successfully built 2ee8069a1ebb\n",
      "Successfully tagged jparacrawl-train:latest\n",
      "The push refers to repository [269376826173.dkr.ecr.us-west-2.amazonaws.com/jparacrawl-train]\n",
      "\n",
      "\u001b[1B83f74970: Preparing \n",
      "\u001b[1Ba9d2c75b: Preparing \n",
      "\u001b[1B76c22096: Preparing \n",
      "\u001b[1B5e983054: Preparing \n",
      "\u001b[1Be2e9a5a0: Preparing \n",
      "\u001b[1Bec63d09d: Preparing \n",
      "\u001b[1B360caa90: Preparing \n",
      "\u001b[1Bb23786a0: Preparing \n",
      "\u001b[1Bd37653c4: Preparing \n",
      "\u001b[1B56016ec1: Preparing \n",
      "\u001b[11B3f74970: Pushed lready exists 9kB1A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[11A\u001b[2Klatest: digest: sha256:c543868df35062096f3f370b71a2e284cd7aaa7a620573cb47d4a28ab4bb1b58 size: 2636\n"
     ]
    }
   ],
   "source": [
    "!./build_and_push.sh jparacrawl-train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16b5a38f-5b8e-4f0b-8f54-dab2469a725d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "role_name = \"SageMaker-local\"\n",
    "\n",
    "iam = boto3.client(\"iam\")\n",
    "role = iam.get_role(RoleName=role_name)[\"Role\"][\"Arn\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acce9eb9-6e80-4bda-b719-7f8d9cab2b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker as sage\n",
    "\n",
    "sess = sage.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ebdc923-c2b0-4d0a-8e2c-db7508a43946",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'jparacrawl/training'\n",
    "WORK_DIRECTORY = 'data_train'\n",
    "data_location = sess.upload_data(WORK_DIRECTORY, key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "59055854-9297-4a52-bd26-cad3cfc63ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-26 02:29:38 Starting - Starting the training job...\n",
      "2022-11-26 02:30:06 Starting - Preparing the instances for trainingProfilerReport-1669429777: InProgress\n",
      "......\n",
      "2022-11-26 02:31:08 Downloading - Downloading input data..............\u001b[34m[2022-11-26 02:33:37,992 INFO] Missing transforms field for corpus_1 data, set to default: [].\u001b[0m\n",
      "\u001b[34m[2022-11-26 02:33:37,992 WARNING] Corpus corpus_1's weight should be given. We default it to 1 for you.\u001b[0m\n",
      "\u001b[34m[2022-11-26 02:33:37,992 INFO] Missing transforms field for valid data, set to default: [].\u001b[0m\n",
      "\u001b[34m[2022-11-26 02:33:37,992 INFO] Parsed 2 corpora from -data.\u001b[0m\n",
      "\u001b[34m[2022-11-26 02:33:37,992 INFO] Get special vocabs from Transforms: {'src': set(), 'tgt': set()}.\u001b[0m\n",
      "\u001b[34m[2022-11-26 02:33:37,992 INFO] Loading vocab from text file...\u001b[0m\n",
      "\u001b[34m[2022-11-26 02:33:37,992 INFO] Loading src vocabulary from /opt/ml/input/data/training/en-ja/jparacrawl.vocab.src\u001b[0m\n",
      "\u001b[34m[2022-11-26 02:33:38,009 INFO] Loaded src vocab has 10532 tokens.\u001b[0m\n",
      "\u001b[34m[2022-11-26 02:33:38,013 INFO] Loading tgt vocabulary from /opt/ml/input/data/training/en-ja/jparacrawl.vocab.tgt\u001b[0m\n",
      "\u001b[34m[2022-11-26 02:33:38,029 INFO] Loaded tgt vocab has 11431 tokens.\u001b[0m\n",
      "\u001b[34m[2022-11-26 02:33:38,033 INFO] Building fields with vocab in counters...\u001b[0m\n",
      "\u001b[34m[2022-11-26 02:33:38,045 INFO]  * tgt vocab size: 11435.\u001b[0m\n",
      "\u001b[34m[2022-11-26 02:33:38,076 INFO]  * src vocab size: 10534.\u001b[0m\n",
      "\u001b[34m[2022-11-26 02:33:38,076 INFO]  * src vocab size = 10534\u001b[0m\n",
      "\u001b[34m[2022-11-26 02:33:38,076 INFO]  * tgt vocab size = 11435\u001b[0m\n",
      "\u001b[34m[2022-11-26 02:33:38,808 INFO]  Starting process pid: 60  \u001b[0m\n",
      "\u001b[34m[2022-11-26 02:33:39,531 INFO]  Starting process pid: 110  \u001b[0m\n",
      "\u001b[34m[2022-11-26 02:33:40,266 INFO]  Starting process pid: 160  \u001b[0m\n",
      "\u001b[34m[2022-11-26 02:33:40,997 INFO]  Starting process pid: 212  \u001b[0m\n",
      "\u001b[34m[2022-11-26 02:33:41,040 INFO] Building model...\u001b[0m\n",
      "\u001b[34m[2022-11-26 02:33:41,820 INFO]  Starting producer process pid: 265  \u001b[0m\n",
      "\u001b[34m[2022-11-26 02:33:41,825 INFO] corpus_1's transforms: TransformPipe()\u001b[0m\n",
      "\u001b[34m[2022-11-26 02:33:41,825 INFO] Weighted corpora loaded so far:\u001b[0m\n",
      "\u001b[34m#011#011#011* corpus_1: 1\u001b[0m\n",
      "\u001b[34m[2022-11-26 02:33:42,606 INFO]  Starting producer process pid: 410  \u001b[0m\n",
      "\u001b[34m[2022-11-26 02:33:43,416 INFO]  Starting producer process pid: 488  \u001b[0m\n",
      "\u001b[34m[2022-11-26 02:33:44,216 INFO]  Starting producer process pid: 562  \u001b[0m\n",
      "\u001b[34m[2022-11-26 02:33:45,576 INFO] NMTModel(\n",
      "  (encoder): TransformerEncoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(10534, 512, padding_idx=1)\n",
      "        )\n",
      "        (pe): PositionalEncoding(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (transformer): ModuleList(\n",
      "      (0): TransformerEncoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): TransformerEncoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): TransformerEncoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (3): TransformerEncoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (4): TransformerEncoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (5): TransformerEncoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "  )\n",
      "  (decoder): TransformerDecoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(11435, 512, padding_idx=1)\n",
      "        )\n",
      "        (pe): PositionalEncoding(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "    (transformer_layers): ModuleList(\n",
      "      (0): TransformerDecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "        (context_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      )\n",
      "      (1): TransformerDecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "        (context_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      )\n",
      "      (2): TransformerDecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "        (context_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      )\n",
      "      (3): TransformerDecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "        (context_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      )\n",
      "      (4): TransformerDecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "        (context_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      )\n",
      "      (5): TransformerDecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "        (context_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (generator): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=11435, bias=True)\n",
      "    (1): Cast()\n",
      "    (2): LogSoftmax(dim=-1)\n",
      "  )\u001b[0m\n",
      "\u001b[34m)\u001b[0m\n",
      "\u001b[34m[2022-11-26 02:33:45,578 INFO] encoder: 24308736\u001b[0m\n",
      "\u001b[34m[2022-11-26 02:33:45,578 INFO] decoder: 36946091\u001b[0m\n",
      "\u001b[34m[2022-11-26 02:33:45,578 INFO] * number of parameters: 61254827\u001b[0m\n",
      "\u001b[34m[2022-11-26 02:33:45,581 INFO] Starting training on GPU: [0, 1, 2, 3]\u001b[0m\n",
      "\u001b[34m[2022-11-26 02:33:45,581 INFO] Start training loop and validate every 10000 steps...\u001b[0m\n",
      "\n",
      "2022-11-26 02:34:03 Training - Training image download completed. Training in progress.\u001b[34m[2022-11-26 02:35:11,049 INFO] Step 100/40000; acc:   5.41; ppl: 2024.11; xent: 7.61; lr: 0.00001; 19733/20990 tok/s;     85 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 02:36:39,633 INFO] Step 200/40000; acc:  10.56; ppl: 656.88; xent: 6.49; lr: 0.00002; 16533/17394 tok/s;    174 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 02:38:13,540 INFO] Step 300/40000; acc:  11.00; ppl: 173.54; xent: 5.16; lr: 0.00004; 20473/24431 tok/s;    268 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 02:39:42,152 INFO] Step 400/40000; acc:  26.55; ppl: 59.76; xent: 4.09; lr: 0.00005; 19066/23803 tok/s;    357 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 02:41:12,221 INFO] Step 500/40000; acc:  29.52; ppl: 52.48; xent: 3.96; lr: 0.00006; 18010/20983 tok/s;    447 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 02:42:43,558 INFO] Step 600/40000; acc:  27.77; ppl: 56.14; xent: 4.03; lr: 0.00007; 15595/18101 tok/s;    538 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 02:44:16,369 INFO] Step 700/40000; acc:  30.31; ppl: 44.15; xent: 3.79; lr: 0.00009; 17528/19996 tok/s;    631 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 02:45:54,481 INFO] Step 800/40000; acc:  30.08; ppl: 35.62; xent: 3.57; lr: 0.00010; 13370/16130 tok/s;    729 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 02:47:26,540 INFO] Step 900/40000; acc:  40.96; ppl: 24.03; xent: 3.18; lr: 0.00011; 18623/18161 tok/s;    821 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 02:49:01,093 INFO] Step 1000/40000; acc:  36.57; ppl: 25.47; xent: 3.24; lr: 0.00012; 18910/20204 tok/s;    916 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 02:50:31,541 INFO] Step 1100/40000; acc:  42.95; ppl: 15.79; xent: 2.76; lr: 0.00014; 19910/25417 tok/s;   1006 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 02:52:00,782 INFO] Step 1200/40000; acc:  49.03; ppl: 12.06; xent: 2.49; lr: 0.00015; 18713/23149 tok/s;   1095 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 02:53:30,830 INFO] Step 1300/40000; acc:  45.88; ppl: 13.47; xent: 2.60; lr: 0.00016; 19197/22358 tok/s;   1185 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 02:55:06,518 INFO] Step 1400/40000; acc:  34.71; ppl: 27.81; xent: 3.33; lr: 0.00017; 17032/19690 tok/s;   1281 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 02:56:40,398 INFO] Step 1500/40000; acc:  39.02; ppl: 20.08; xent: 3.00; lr: 0.00019; 16516/18836 tok/s;   1375 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 02:58:13,378 INFO] Step 1600/40000; acc:  36.53; ppl: 23.14; xent: 3.14; lr: 0.00020; 16725/18579 tok/s;   1468 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 02:59:45,348 INFO] Step 1700/40000; acc:  37.85; ppl: 20.76; xent: 3.03; lr: 0.00021; 16280/17802 tok/s;   1560 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 03:01:17,012 INFO] Step 1800/40000; acc:  56.07; ppl:  7.30; xent: 1.99; lr: 0.00022; 19405/23883 tok/s;   1651 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 03:02:48,913 INFO] Step 1900/40000; acc:  43.28; ppl: 14.73; xent: 2.69; lr: 0.00023; 17477/19426 tok/s;   1743 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 03:04:21,667 INFO] Step 2000/40000; acc:  38.52; ppl: 17.94; xent: 2.89; lr: 0.00025; 16431/18134 tok/s;   1836 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 03:05:53,697 INFO] Step 2100/40000; acc:  37.52; ppl: 18.61; xent: 2.92; lr: 0.00026; 16502/17944 tok/s;   1928 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 03:07:25,851 INFO] Step 2200/40000; acc:  41.58; ppl: 14.89; xent: 2.70; lr: 0.00027; 16589/18070 tok/s;   2020 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 03:08:58,969 INFO] Step 2300/40000; acc:  62.62; ppl:  4.97; xent: 1.60; lr: 0.00028; 19462/23655 tok/s;   2113 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 03:10:27,704 INFO] Step 2400/40000; acc:  61.05; ppl:  5.42; xent: 1.69; lr: 0.00030; 17880/21439 tok/s;   2202 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 03:11:51,574 INFO] Step 2500/40000; acc:  44.54; ppl: 12.36; xent: 2.51; lr: 0.00031; 18058/20518 tok/s;   2286 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 03:13:14,455 INFO] Step 2600/40000; acc:  44.22; ppl: 12.51; xent: 2.53; lr: 0.00032; 18514/20127 tok/s;   2369 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 03:14:38,519 INFO] Step 2700/40000; acc:  43.95; ppl: 12.25; xent: 2.51; lr: 0.00033; 18163/19636 tok/s;   2453 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 03:16:02,523 INFO] Step 2800/40000; acc:  48.33; ppl:  9.87; xent: 2.29; lr: 0.00035; 18827/20459 tok/s;   2537 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 03:17:24,507 INFO] Step 2900/40000; acc:  47.98; ppl: 10.14; xent: 2.32; lr: 0.00036; 18480/21494 tok/s;   2619 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 03:18:48,680 INFO] Step 3000/40000; acc:  46.86; ppl: 10.38; xent: 2.34; lr: 0.00037; 17598/20708 tok/s;   2703 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 03:20:17,644 INFO] Step 3100/40000; acc:  50.37; ppl:  8.61; xent: 2.15; lr: 0.00038; 17968/20818 tok/s;   2792 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 03:21:40,340 INFO] Step 3200/40000; acc:  50.92; ppl:  8.31; xent: 2.12; lr: 0.00040; 18426/21511 tok/s;   2875 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 03:23:03,742 INFO] Step 3300/40000; acc:  49.43; ppl:  9.23; xent: 2.22; lr: 0.00041; 17985/20452 tok/s;   2958 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 03:24:27,545 INFO] Step 3400/40000; acc:  50.36; ppl:  8.68; xent: 2.16; lr: 0.00042; 18032/20535 tok/s;   3042 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 03:25:50,479 INFO] Step 3500/40000; acc:  47.12; ppl: 10.13; xent: 2.32; lr: 0.00043; 17721/19769 tok/s;   3125 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 03:27:16,817 INFO] Step 3600/40000; acc:  70.65; ppl:  3.24; xent: 1.17; lr: 0.00044; 21997/25977 tok/s;   3211 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 03:28:44,563 INFO] Step 3700/40000; acc:  76.32; ppl:  2.55; xent: 0.93; lr: 0.00046; 19165/23957 tok/s;   3299 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 03:30:14,443 INFO] Step 3800/40000; acc:  67.12; ppl:  3.90; xent: 1.36; lr: 0.00047; 18102/21403 tok/s;   3389 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 03:31:46,137 INFO] Step 3900/40000; acc:  51.98; ppl:  7.83; xent: 2.06; lr: 0.00048; 16584/18292 tok/s;   3481 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 03:33:21,155 INFO] Step 4000/40000; acc:  48.75; ppl:  9.14; xent: 2.21; lr: 0.00049; 15742/18183 tok/s;   3576 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 03:34:53,804 INFO] Step 4100/40000; acc:  47.23; ppl: 10.05; xent: 2.31; lr: 0.00051; 16240/18090 tok/s;   3668 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 03:36:25,414 INFO] Step 4200/40000; acc:  46.06; ppl: 10.53; xent: 2.35; lr: 0.00052; 16058/18398 tok/s;   3760 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 03:37:57,870 INFO] Step 4300/40000; acc:  51.07; ppl:  8.09; xent: 2.09; lr: 0.00053; 17837/19701 tok/s;   3852 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 03:39:25,708 INFO] Step 4400/40000; acc:  58.14; ppl:  6.30; xent: 1.84; lr: 0.00054; 18199/19976 tok/s;   3940 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 03:40:49,896 INFO] Step 4500/40000; acc:  62.53; ppl:  4.66; xent: 1.54; lr: 0.00056; 20220/22506 tok/s;   4024 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 03:42:12,476 INFO] Step 4600/40000; acc:  58.02; ppl:  5.77; xent: 1.75; lr: 0.00057; 18427/22032 tok/s;   4107 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 03:43:37,801 INFO] Step 4700/40000; acc:  49.46; ppl:  8.51; xent: 2.14; lr: 0.00058; 18519/20132 tok/s;   4192 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 03:45:05,410 INFO] Step 4800/40000; acc:  49.09; ppl:  8.63; xent: 2.16; lr: 0.00059; 17313/19796 tok/s;   4280 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 03:46:29,164 INFO] Step 4900/40000; acc:  49.03; ppl:  8.95; xent: 2.19; lr: 0.00061; 17006/19245 tok/s;   4364 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 03:47:51,928 INFO] Step 5000/40000; acc:  48.38; ppl:  9.15; xent: 2.21; lr: 0.00062; 18115/19868 tok/s;   4446 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 03:49:15,887 INFO] Step 5100/40000; acc:  51.17; ppl:  7.98; xent: 2.08; lr: 0.00063; 16715/18603 tok/s;   4530 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 03:50:43,624 INFO] Step 5200/40000; acc:  68.01; ppl:  3.85; xent: 1.35; lr: 0.00064; 19027/21284 tok/s;   4618 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 03:52:12,652 INFO] Step 5300/40000; acc:  59.23; ppl:  5.34; xent: 1.68; lr: 0.00065; 18644/20774 tok/s;   4707 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 03:53:45,219 INFO] Step 5400/40000; acc:  65.95; ppl:  4.41; xent: 1.48; lr: 0.00067; 19451/19616 tok/s;   4800 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 03:55:09,216 INFO] Step 5500/40000; acc:  65.38; ppl:  4.04; xent: 1.40; lr: 0.00068; 19558/21686 tok/s;   4884 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 03:56:37,458 INFO] Step 5600/40000; acc:  70.22; ppl:  3.20; xent: 1.16; lr: 0.00069; 19903/23157 tok/s;   4972 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 03:58:07,612 INFO] Step 5700/40000; acc:  74.90; ppl:  2.59; xent: 0.95; lr: 0.00070; 19810/23926 tok/s;   5062 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 03:59:39,413 INFO] Step 5800/40000; acc:  64.00; ppl:  4.19; xent: 1.43; lr: 0.00072; 17943/20072 tok/s;   5154 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 04:01:12,941 INFO] Step 5900/40000; acc:  67.41; ppl:  3.64; xent: 1.29; lr: 0.00073; 18604/20543 tok/s;   5247 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 04:02:45,934 INFO] Step 6000/40000; acc:  65.60; ppl:  4.05; xent: 1.40; lr: 0.00074; 18482/20069 tok/s;   5340 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 04:04:23,018 INFO] Step 6100/40000; acc:  66.25; ppl:  4.18; xent: 1.43; lr: 0.00075; 17706/18616 tok/s;   5437 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 04:05:53,976 INFO] Step 6200/40000; acc:  56.47; ppl:  6.26; xent: 1.83; lr: 0.00077; 16610/18897 tok/s;   5528 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 04:07:28,760 INFO] Step 6300/40000; acc:  63.54; ppl:  4.75; xent: 1.56; lr: 0.00078; 17911/20009 tok/s;   5623 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 04:09:02,682 INFO] Step 6400/40000; acc:  61.81; ppl:  5.14; xent: 1.64; lr: 0.00079; 16605/18393 tok/s;   5717 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 04:10:38,042 INFO] Step 6500/40000; acc:  52.41; ppl:  7.30; xent: 1.99; lr: 0.00080; 16864/18425 tok/s;   5812 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 04:12:09,673 INFO] Step 6600/40000; acc:  50.99; ppl:  8.31; xent: 2.12; lr: 0.00082; 16286/17383 tok/s;   5904 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 04:13:42,501 INFO] Step 6700/40000; acc:  53.50; ppl:  6.89; xent: 1.93; lr: 0.00083; 16706/18339 tok/s;   5997 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 04:15:13,478 INFO] Step 6800/40000; acc:  56.20; ppl:  6.09; xent: 1.81; lr: 0.00084; 16606/18735 tok/s;   6088 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 04:16:46,579 INFO] Step 6900/40000; acc:  66.13; ppl:  4.06; xent: 1.40; lr: 0.00085; 17759/19514 tok/s;   6181 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 04:18:19,134 INFO] Step 7000/40000; acc:  75.90; ppl:  2.51; xent: 0.92; lr: 0.00086; 20360/24702 tok/s;   6274 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 04:19:48,281 INFO] Step 7100/40000; acc:  79.73; ppl:  2.15; xent: 0.76; lr: 0.00088; 18950/23686 tok/s;   6363 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 04:21:19,841 INFO] Step 7200/40000; acc:  70.11; ppl:  3.27; xent: 1.18; lr: 0.00089; 18368/20970 tok/s;   6454 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 04:22:52,117 INFO] Step 7300/40000; acc:  58.47; ppl:  5.64; xent: 1.73; lr: 0.00090; 15086/17308 tok/s;   6547 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 04:24:23,345 INFO] Step 7400/40000; acc:  56.71; ppl:  5.97; xent: 1.79; lr: 0.00091; 16732/18431 tok/s;   6638 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 04:25:56,607 INFO] Step 7500/40000; acc:  64.88; ppl:  4.41; xent: 1.48; lr: 0.00093; 17412/19230 tok/s;   6731 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 04:27:25,991 INFO] Step 7600/40000; acc:  54.84; ppl:  6.21; xent: 1.83; lr: 0.00094; 17863/19463 tok/s;   6820 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 04:29:00,317 INFO] Step 7700/40000; acc:  63.17; ppl:  4.52; xent: 1.51; lr: 0.00095; 17527/19488 tok/s;   6915 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 04:30:31,020 INFO] Step 7800/40000; acc:  78.85; ppl:  2.21; xent: 0.79; lr: 0.00096; 20058/25263 tok/s;   7005 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 04:31:54,102 INFO] Step 7900/40000; acc:  80.00; ppl:  2.11; xent: 0.75; lr: 0.00098; 20112/24545 tok/s;   7089 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 04:33:19,127 INFO] Step 8000/40000; acc:  56.24; ppl:  6.13; xent: 1.81; lr: 0.00099; 19079/20880 tok/s;   7174 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 04:34:42,226 INFO] Step 8100/40000; acc:  50.42; ppl:  8.57; xent: 2.15; lr: 0.00098; 15944/18695 tok/s;   7257 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 04:36:06,284 INFO] Step 8200/40000; acc:  50.88; ppl:  7.87; xent: 2.06; lr: 0.00098; 17374/19583 tok/s;   7341 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 04:37:28,418 INFO] Step 8300/40000; acc:  77.24; ppl:  2.36; xent: 0.86; lr: 0.00097; 22075/26822 tok/s;   7423 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 04:38:49,223 INFO] Step 8400/40000; acc:  74.11; ppl:  2.79; xent: 1.02; lr: 0.00096; 20128/23950 tok/s;   7504 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 04:40:12,452 INFO] Step 8500/40000; acc:  53.28; ppl:  6.97; xent: 1.94; lr: 0.00096; 18068/20321 tok/s;   7587 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 04:41:33,312 INFO] Step 8600/40000; acc:  61.16; ppl:  4.97; xent: 1.60; lr: 0.00095; 17872/20527 tok/s;   7668 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 04:42:59,377 INFO] Step 8700/40000; acc:  51.89; ppl:  7.32; xent: 1.99; lr: 0.00095; 18238/19949 tok/s;   7754 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 04:44:29,414 INFO] Step 8800/40000; acc:  58.14; ppl:  5.46; xent: 1.70; lr: 0.00094; 18095/19971 tok/s;   7844 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 04:46:02,645 INFO] Step 8900/40000; acc:  53.19; ppl:  6.96; xent: 1.94; lr: 0.00094; 17993/18132 tok/s;   7937 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 04:47:36,217 INFO] Step 9000/40000; acc:  69.52; ppl:  3.30; xent: 1.19; lr: 0.00093; 19040/20646 tok/s;   8031 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 04:49:05,048 INFO] Step 9100/40000; acc:  79.03; ppl:  2.21; xent: 0.79; lr: 0.00093; 18896/23296 tok/s;   8119 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 04:50:39,467 INFO] Step 9200/40000; acc:  56.14; ppl:  6.11; xent: 1.81; lr: 0.00092; 15815/17466 tok/s;   8214 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 04:52:12,414 INFO] Step 9300/40000; acc:  56.10; ppl:  5.81; xent: 1.76; lr: 0.00092; 16683/18445 tok/s;   8307 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 04:53:35,192 INFO] Step 9400/40000; acc:  59.08; ppl:  5.19; xent: 1.65; lr: 0.00091; 16769/18399 tok/s;   8390 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 04:55:01,569 INFO] Step 9500/40000; acc:  73.33; ppl:  2.60; xent: 0.95; lr: 0.00091; 20818/23390 tok/s;   8476 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 04:56:22,339 INFO] Step 9600/40000; acc:  78.85; ppl:  2.09; xent: 0.74; lr: 0.00090; 19042/24135 tok/s;   8557 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 04:57:47,478 INFO] Step 9700/40000; acc:  76.47; ppl:  2.36; xent: 0.86; lr: 0.00090; 16849/21489 tok/s;   8642 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 04:59:18,977 INFO] Step 9800/40000; acc:  61.84; ppl:  4.75; xent: 1.56; lr: 0.00089; 17201/19182 tok/s;   8733 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 05:00:52,206 INFO] Step 9900/40000; acc:  57.66; ppl:  5.18; xent: 1.65; lr: 0.00089; 15176/17275 tok/s;   8827 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 05:02:22,713 INFO] Step 10000/40000; acc:  59.33; ppl:  5.29; xent: 1.67; lr: 0.00088; 16932/19324 tok/s;   8917 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 05:02:22,714 INFO] valid's transforms: TransformPipe()\u001b[0m\n",
      "\u001b[34m[2022-11-26 05:02:22,820 WARNING] The batch will be filled until we reach 1,its size may exceed 32 tokens\u001b[0m\n",
      "\u001b[34m[2022-11-26 05:02:22,821 WARNING] The batch will be filled until we reach 1,its size may exceed 32 tokens\u001b[0m\n",
      "\u001b[34m[2022-11-26 05:02:22,824 WARNING] The batch will be filled until we reach 1,its size may exceed 32 tokens\u001b[0m\n",
      "\u001b[34m[2022-11-26 05:02:48,966 WARNING] The batch will be filled until we reach 1,its size may exceed 32 tokens\u001b[0m\n",
      "\u001b[34m[2022-11-26 05:02:48,967 WARNING] The batch will be filled until we reach 1,its size may exceed 32 tokens\u001b[0m\n",
      "\u001b[34m[2022-11-26 05:02:48,968 WARNING] The batch will be filled until we reach 1,its size may exceed 32 tokens\u001b[0m\n",
      "\u001b[34m[2022-11-26 05:02:48,969 WARNING] The batch will be filled until we reach 1,its size may exceed 32 tokens\u001b[0m\n",
      "\u001b[34m[2022-11-26 05:02:48,970 WARNING] The batch will be filled until we reach 1,its size may exceed 32 tokens\u001b[0m\n",
      "\u001b[34m[2022-11-26 05:03:27,492 INFO] Validation perplexity: 9.86905\u001b[0m\n",
      "\u001b[34m[2022-11-26 05:03:27,492 INFO] Validation accuracy: 53.3684\u001b[0m\n",
      "\u001b[34m[2022-11-26 05:03:27,569 INFO] Saving checkpoint /opt/ml/model/model.en-ja_step_10000.pt\u001b[0m\n",
      "\u001b[34m[2022-11-26 05:05:01,980 INFO] Step 10100/40000; acc:  65.23; ppl:  4.03; xent: 1.39; lr: 0.00088; 10003/11299 tok/s;   9076 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 05:06:27,994 INFO] Step 10200/40000; acc:  63.83; ppl:  4.18; xent: 1.43; lr: 0.00088; 17728/20594 tok/s;   9162 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 05:07:50,115 INFO] Step 10300/40000; acc:  72.14; ppl:  2.88; xent: 1.06; lr: 0.00087; 19305/23386 tok/s;   9245 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 05:09:13,513 INFO] Step 10400/40000; acc:  59.76; ppl:  5.25; xent: 1.66; lr: 0.00087; 18833/20282 tok/s;   9328 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 05:10:39,598 INFO] Step 10500/40000; acc:  55.47; ppl:  6.34; xent: 1.85; lr: 0.00086; 17640/19137 tok/s;   9414 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 05:12:06,439 INFO] Step 10600/40000; acc:  61.55; ppl:  4.73; xent: 1.55; lr: 0.00086; 17976/19859 tok/s;   9501 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 05:13:29,732 INFO] Step 10700/40000; acc:  58.07; ppl:  5.59; xent: 1.72; lr: 0.00085; 17909/20662 tok/s;   9584 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 05:14:55,893 INFO] Step 10800/40000; acc:  54.10; ppl:  6.73; xent: 1.91; lr: 0.00085; 17514/18735 tok/s;   9670 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 05:16:22,152 INFO] Step 10900/40000; acc:  51.28; ppl:  7.53; xent: 2.02; lr: 0.00085; 17330/19785 tok/s;   9757 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 05:17:54,496 INFO] Step 11000/40000; acc:  55.51; ppl:  6.24; xent: 1.83; lr: 0.00084; 16133/18108 tok/s;   9849 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 05:19:26,703 INFO] Step 11100/40000; acc:  56.95; ppl:  5.79; xent: 1.76; lr: 0.00084; 16101/17622 tok/s;   9941 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 05:20:57,976 INFO] Step 11200/40000; acc:  57.50; ppl:  5.67; xent: 1.74; lr: 0.00084; 16292/18497 tok/s;  10032 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 05:22:32,383 INFO] Step 11300/40000; acc:  58.85; ppl:  5.30; xent: 1.67; lr: 0.00083; 15956/18136 tok/s;  10127 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 05:24:05,087 INFO] Step 11400/40000; acc:  54.83; ppl:  6.30; xent: 1.84; lr: 0.00083; 15192/17567 tok/s;  10220 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 05:25:39,357 INFO] Step 11500/40000; acc:  66.39; ppl:  3.77; xent: 1.33; lr: 0.00082; 17035/19297 tok/s;  10314 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 05:27:09,383 INFO] Step 11600/40000; acc:  78.89; ppl:  2.17; xent: 0.77; lr: 0.00082; 19860/24813 tok/s;  10404 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 05:28:39,104 INFO] Step 11700/40000; acc:  77.20; ppl:  2.37; xent: 0.86; lr: 0.00082; 18293/21828 tok/s;  10494 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 05:30:14,740 INFO] Step 11800/40000; acc:  58.17; ppl:  5.76; xent: 1.75; lr: 0.00081; 15938/16874 tok/s;  10589 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 05:31:41,731 INFO] Step 11900/40000; acc:  50.28; ppl:  8.06; xent: 2.09; lr: 0.00081; 16188/17294 tok/s;  10676 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 05:33:13,126 INFO] Step 12000/40000; acc:  50.02; ppl:  7.94; xent: 2.07; lr: 0.00081; 15804/17256 tok/s;  10768 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 05:34:50,655 INFO] Step 12100/40000; acc:  64.53; ppl:  4.26; xent: 1.45; lr: 0.00080; 19274/18291 tok/s;  10865 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 05:36:25,059 INFO] Step 12200/40000; acc:  58.11; ppl:  5.78; xent: 1.75; lr: 0.00080; 15851/16658 tok/s;  10959 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 05:37:57,518 INFO] Step 12300/40000; acc:  58.65; ppl:  5.54; xent: 1.71; lr: 0.00080; 15870/17524 tok/s;  11052 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 05:39:33,648 INFO] Step 12400/40000; acc:  60.48; ppl:  5.31; xent: 1.67; lr: 0.00079; 18401/17206 tok/s;  11148 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 05:41:10,938 INFO] Step 12500/40000; acc:  59.02; ppl:  5.57; xent: 1.72; lr: 0.00079; 17388/17593 tok/s;  11245 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 05:42:41,436 INFO] Step 12600/40000; acc:  54.85; ppl:  6.43; xent: 1.86; lr: 0.00079; 16522/17813 tok/s;  11336 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 05:44:04,145 INFO] Step 12700/40000; acc:  55.00; ppl:  6.60; xent: 1.89; lr: 0.00078; 17722/18774 tok/s;  11419 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 05:45:27,519 INFO] Step 12800/40000; acc:  53.71; ppl:  6.86; xent: 1.93; lr: 0.00078; 16756/18455 tok/s;  11502 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 05:46:53,462 INFO] Step 12900/40000; acc:  64.24; ppl:  4.34; xent: 1.47; lr: 0.00078; 17640/19559 tok/s;  11588 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 05:48:21,813 INFO] Step 13000/40000; acc:  78.43; ppl:  2.22; xent: 0.80; lr: 0.00078; 21666/25669 tok/s;  11676 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 05:49:42,335 INFO] Step 13100/40000; acc:  81.67; ppl:  1.95; xent: 0.67; lr: 0.00077; 21520/27242 tok/s;  11757 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 05:51:03,402 INFO] Step 13200/40000; acc:  78.74; ppl:  2.23; xent: 0.80; lr: 0.00077; 20716/24848 tok/s;  11838 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 05:52:30,238 INFO] Step 13300/40000; acc:  58.76; ppl:  5.33; xent: 1.67; lr: 0.00077; 17264/19125 tok/s;  11925 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 05:54:01,096 INFO] Step 13400/40000; acc:  53.87; ppl:  6.31; xent: 1.84; lr: 0.00076; 18042/20227 tok/s;  12016 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 05:55:31,419 INFO] Step 13500/40000; acc:  59.30; ppl:  5.12; xent: 1.63; lr: 0.00076; 16190/17970 tok/s;  12106 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 05:57:05,117 INFO] Step 13600/40000; acc:  76.93; ppl:  2.34; xent: 0.85; lr: 0.00076; 20222/23562 tok/s;  12200 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 05:58:33,978 INFO] Step 13700/40000; acc:  80.30; ppl:  2.05; xent: 0.72; lr: 0.00076; 18953/23320 tok/s;  12288 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 06:00:05,034 INFO] Step 13800/40000; acc:  66.52; ppl:  3.74; xent: 1.32; lr: 0.00075; 18148/20536 tok/s;  12379 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 06:01:35,527 INFO] Step 13900/40000; acc:  54.91; ppl:  6.31; xent: 1.84; lr: 0.00075; 16143/17473 tok/s;  12470 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 06:03:07,704 INFO] Step 14000/40000; acc:  53.44; ppl:  6.90; xent: 1.93; lr: 0.00075; 15334/17302 tok/s;  12562 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 06:04:40,879 INFO] Step 14100/40000; acc:  54.64; ppl:  6.44; xent: 1.86; lr: 0.00074; 16082/17371 tok/s;  12655 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 06:06:13,925 INFO] Step 14200/40000; acc:  60.52; ppl:  4.88; xent: 1.59; lr: 0.00074; 16887/19043 tok/s;  12748 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 06:07:46,027 INFO] Step 14300/40000; acc:  68.89; ppl:  3.43; xent: 1.23; lr: 0.00074; 17342/20001 tok/s;  12840 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 06:09:18,445 INFO] Step 14400/40000; acc:  56.81; ppl:  5.75; xent: 1.75; lr: 0.00074; 17279/18222 tok/s;  12933 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 06:10:49,343 INFO] Step 14500/40000; acc:  62.39; ppl:  4.64; xent: 1.53; lr: 0.00073; 16599/18085 tok/s;  13024 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 06:12:22,593 INFO] Step 14600/40000; acc:  62.23; ppl:  4.29; xent: 1.46; lr: 0.00073; 17840/19380 tok/s;  13117 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 06:13:47,005 INFO] Step 14700/40000; acc:  74.75; ppl:  2.57; xent: 0.94; lr: 0.00073; 20659/24404 tok/s;  13201 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 06:15:08,097 INFO] Step 14800/40000; acc:  74.93; ppl:  2.60; xent: 0.96; lr: 0.00073; 20027/23521 tok/s;  13283 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 06:16:33,257 INFO] Step 14900/40000; acc:  70.61; ppl:  3.00; xent: 1.10; lr: 0.00072; 20852/23147 tok/s;  13368 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 06:17:57,368 INFO] Step 15000/40000; acc:  80.41; ppl:  2.03; xent: 0.71; lr: 0.00072; 20703/25779 tok/s;  13452 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 06:19:28,202 INFO] Step 15100/40000; acc:  70.96; ppl:  3.18; xent: 1.16; lr: 0.00072; 17812/20352 tok/s;  13543 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 06:21:00,651 INFO] Step 15200/40000; acc:  58.81; ppl:  5.46; xent: 1.70; lr: 0.00072; 16507/18112 tok/s;  13635 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 06:22:32,526 INFO] Step 15300/40000; acc:  56.81; ppl:  5.91; xent: 1.78; lr: 0.00071; 16951/17820 tok/s;  13727 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 06:24:03,990 INFO] Step 15400/40000; acc:  62.55; ppl:  4.66; xent: 1.54; lr: 0.00071; 16648/18282 tok/s;  13818 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 06:25:34,991 INFO] Step 15500/40000; acc:  59.60; ppl:  5.12; xent: 1.63; lr: 0.00071; 17221/18789 tok/s;  13909 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 06:27:07,442 INFO] Step 15600/40000; acc:  58.36; ppl:  5.34; xent: 1.68; lr: 0.00071; 16339/17709 tok/s;  14002 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 06:28:38,203 INFO] Step 15700/40000; acc:  65.52; ppl:  3.95; xent: 1.37; lr: 0.00071; 16472/18877 tok/s;  14093 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 06:30:12,607 INFO] Step 15800/40000; acc:  77.33; ppl:  2.29; xent: 0.83; lr: 0.00070; 20143/23392 tok/s;  14187 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 06:31:40,649 INFO] Step 15900/40000; acc:  80.75; ppl:  2.00; xent: 0.69; lr: 0.00070; 19677/24568 tok/s;  14275 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 06:33:10,730 INFO] Step 16000/40000; acc:  76.29; ppl:  2.44; xent: 0.89; lr: 0.00070; 19351/22891 tok/s;  14365 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 06:34:42,419 INFO] Step 16100/40000; acc:  64.83; ppl:  3.98; xent: 1.38; lr: 0.00070; 17906/19901 tok/s;  14457 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 06:36:04,360 INFO] Step 16200/40000; acc:  81.77; ppl:  1.92; xent: 0.65; lr: 0.00069; 21647/27000 tok/s;  14539 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 06:37:27,285 INFO] Step 16300/40000; acc:  69.34; ppl:  3.39; xent: 1.22; lr: 0.00069; 18838/21758 tok/s;  14622 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 06:38:53,597 INFO] Step 16400/40000; acc:  59.36; ppl:  5.24; xent: 1.66; lr: 0.00069; 17819/19017 tok/s;  14708 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 06:40:17,940 INFO] Step 16500/40000; acc:  61.58; ppl:  4.81; xent: 1.57; lr: 0.00069; 18426/19858 tok/s;  14792 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 06:41:46,209 INFO] Step 16600/40000; acc:  70.99; ppl:  3.22; xent: 1.17; lr: 0.00069; 18139/20470 tok/s;  14881 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 06:43:12,194 INFO] Step 16700/40000; acc:  54.35; ppl:  6.43; xent: 1.86; lr: 0.00068; 17870/19662 tok/s;  14967 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 06:44:35,979 INFO] Step 16800/40000; acc:  57.08; ppl:  5.76; xent: 1.75; lr: 0.00068; 18780/19531 tok/s;  15050 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 06:46:00,138 INFO] Step 16900/40000; acc:  59.20; ppl:  5.19; xent: 1.65; lr: 0.00068; 18277/20795 tok/s;  15135 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 06:47:28,009 INFO] Step 17000/40000; acc:  54.97; ppl:  6.20; xent: 1.83; lr: 0.00068; 17622/20103 tok/s;  15222 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 06:48:57,343 INFO] Step 17100/40000; acc:  54.53; ppl:  6.20; xent: 1.82; lr: 0.00068; 16642/19066 tok/s;  15312 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 06:50:28,387 INFO] Step 17200/40000; acc:  55.94; ppl:  5.85; xent: 1.77; lr: 0.00067; 17496/19081 tok/s;  15403 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 06:51:56,815 INFO] Step 17300/40000; acc:  63.80; ppl:  4.32; xent: 1.46; lr: 0.00067; 18244/18746 tok/s;  15491 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 06:53:18,984 INFO] Step 17400/40000; acc:  68.95; ppl:  3.43; xent: 1.23; lr: 0.00067; 18375/21821 tok/s;  15573 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 06:54:43,124 INFO] Step 17500/40000; acc:  58.24; ppl:  5.39; xent: 1.68; lr: 0.00067; 18347/19071 tok/s;  15658 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 06:56:10,263 INFO] Step 17600/40000; acc:  56.05; ppl:  5.66; xent: 1.73; lr: 0.00067; 17464/20345 tok/s;  15745 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 06:57:32,661 INFO] Step 17700/40000; acc:  58.53; ppl:  5.19; xent: 1.65; lr: 0.00066; 18349/20992 tok/s;  15827 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 06:58:55,842 INFO] Step 17800/40000; acc:  61.59; ppl:  4.73; xent: 1.55; lr: 0.00066; 18574/20409 tok/s;  15910 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 07:00:21,139 INFO] Step 17900/40000; acc:  52.03; ppl:  6.73; xent: 1.91; lr: 0.00066; 20332/20189 tok/s;  15996 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 07:01:44,499 INFO] Step 18000/40000; acc:  58.09; ppl:  5.08; xent: 1.63; lr: 0.00066; 19945/21646 tok/s;  16079 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 07:03:07,237 INFO] Step 18100/40000; acc:  59.05; ppl:  5.37; xent: 1.68; lr: 0.00066; 18667/20324 tok/s;  16162 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 07:04:31,441 INFO] Step 18200/40000; acc:  57.07; ppl:  5.51; xent: 1.71; lr: 0.00066; 19500/20211 tok/s;  16246 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 07:05:54,898 INFO] Step 18300/40000; acc:  69.42; ppl:  3.29; xent: 1.19; lr: 0.00065; 18974/20851 tok/s;  16329 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 07:07:17,396 INFO] Step 18400/40000; acc:  80.23; ppl:  2.04; xent: 0.71; lr: 0.00065; 23047/27950 tok/s;  16412 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 07:08:37,638 INFO] Step 18500/40000; acc:  82.55; ppl:  1.87; xent: 0.63; lr: 0.00065; 21429/27181 tok/s;  16492 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 07:09:58,635 INFO] Step 18600/40000; acc:  81.38; ppl:  1.97; xent: 0.68; lr: 0.00065; 20680/24969 tok/s;  16573 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 07:11:21,429 INFO] Step 18700/40000; acc:  65.50; ppl:  4.00; xent: 1.39; lr: 0.00065; 20158/22887 tok/s;  16656 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 07:12:45,358 INFO] Step 18800/40000; acc:  61.06; ppl:  4.73; xent: 1.55; lr: 0.00064; 18231/20638 tok/s;  16740 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 07:14:10,544 INFO] Step 18900/40000; acc:  67.40; ppl:  3.76; xent: 1.33; lr: 0.00064; 17636/20303 tok/s;  16825 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 07:15:40,149 INFO] Step 19000/40000; acc:  62.71; ppl:  4.59; xent: 1.52; lr: 0.00064; 16919/19083 tok/s;  16915 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 07:17:11,497 INFO] Step 19100/40000; acc:  60.08; ppl:  5.10; xent: 1.63; lr: 0.00064; 16880/19028 tok/s;  17006 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 07:18:44,575 INFO] Step 19200/40000; acc:  69.37; ppl:  3.27; xent: 1.18; lr: 0.00064; 18448/20032 tok/s;  17099 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 07:20:13,778 INFO] Step 19300/40000; acc:  63.68; ppl:  4.45; xent: 1.49; lr: 0.00064; 16790/18404 tok/s;  17188 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 07:21:40,635 INFO] Step 19400/40000; acc:  63.46; ppl:  4.48; xent: 1.50; lr: 0.00063; 17443/19664 tok/s;  17275 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 07:23:04,418 INFO] Step 19500/40000; acc:  58.56; ppl:  5.28; xent: 1.66; lr: 0.00063; 18679/20205 tok/s;  17359 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 07:24:28,697 INFO] Step 19600/40000; acc:  75.30; ppl:  2.53; xent: 0.93; lr: 0.00063; 20638/24169 tok/s;  17443 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 07:25:49,431 INFO] Step 19700/40000; acc:  82.18; ppl:  1.89; xent: 0.64; lr: 0.00063; 20566/25653 tok/s;  17524 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 07:27:12,979 INFO] Step 19800/40000; acc:  70.88; ppl:  3.16; xent: 1.15; lr: 0.00063; 18962/21777 tok/s;  17607 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 07:28:35,337 INFO] Step 19900/40000; acc:  61.34; ppl:  4.83; xent: 1.57; lr: 0.00063; 18672/20329 tok/s;  17690 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 07:30:00,363 INFO] Step 20000/40000; acc:  59.51; ppl:  5.25; xent: 1.66; lr: 0.00062; 17361/19224 tok/s;  17775 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 07:30:00,462 WARNING] The batch will be filled until we reach 1,its size may exceed 32 tokens\u001b[0m\n",
      "\u001b[34m[2022-11-26 07:30:00,462 WARNING] The batch will be filled until we reach 1,its size may exceed 32 tokens\u001b[0m\n",
      "\u001b[34m[2022-11-26 07:30:00,466 WARNING] The batch will be filled until we reach 1,its size may exceed 32 tokens\u001b[0m\n",
      "\u001b[34m[2022-11-26 07:30:21,078 WARNING] The batch will be filled until we reach 1,its size may exceed 32 tokens\u001b[0m\n",
      "\u001b[34m[2022-11-26 07:30:21,079 WARNING] The batch will be filled until we reach 1,its size may exceed 32 tokens\u001b[0m\n",
      "\u001b[34m[2022-11-26 07:30:21,080 WARNING] The batch will be filled until we reach 1,its size may exceed 32 tokens\u001b[0m\n",
      "\u001b[34m[2022-11-26 07:30:21,081 WARNING] The batch will be filled until we reach 1,its size may exceed 32 tokens\u001b[0m\n",
      "\u001b[34m[2022-11-26 07:30:21,082 WARNING] The batch will be filled until we reach 1,its size may exceed 32 tokens\u001b[0m\n",
      "\u001b[34m[2022-11-26 07:30:51,434 INFO] Validation perplexity: 7.18223\u001b[0m\n",
      "\u001b[34m[2022-11-26 07:30:51,434 INFO] Validation accuracy: 58.2573\u001b[0m\n",
      "\u001b[34m[2022-11-26 07:30:51,511 INFO] Saving checkpoint /opt/ml/model/model.en-ja_step_20000.pt\u001b[0m\n",
      "\u001b[34m[2022-11-26 07:32:17,089 INFO] Step 20100/40000; acc:  59.37; ppl:  5.47; xent: 1.70; lr: 0.00062; 10334/11690 tok/s;  17912 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 07:33:51,052 INFO] Step 20200/40000; acc:  58.96; ppl:  5.15; xent: 1.64; lr: 0.00062; 16170/18343 tok/s;  18005 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 07:35:24,217 INFO] Step 20300/40000; acc:  57.53; ppl:  5.57; xent: 1.72; lr: 0.00062; 15987/17815 tok/s;  18099 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 07:36:55,417 INFO] Step 20400/40000; acc:  56.76; ppl:  5.84; xent: 1.76; lr: 0.00062; 16426/17928 tok/s;  18190 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 07:38:27,828 INFO] Step 20500/40000; acc:  64.73; ppl:  4.20; xent: 1.44; lr: 0.00062; 17183/18524 tok/s;  18282 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 07:40:00,494 INFO] Step 20600/40000; acc:  60.98; ppl:  4.90; xent: 1.59; lr: 0.00062; 16499/18177 tok/s;  18375 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 07:41:33,372 INFO] Step 20700/40000; acc:  61.83; ppl:  4.67; xent: 1.54; lr: 0.00061; 16258/17914 tok/s;  18468 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 07:43:06,264 INFO] Step 20800/40000; acc:  72.98; ppl:  3.00; xent: 1.10; lr: 0.00061; 17015/19710 tok/s;  18561 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 07:44:38,105 INFO] Step 20900/40000; acc:  60.77; ppl:  4.99; xent: 1.61; lr: 0.00061; 16584/18105 tok/s;  18653 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 07:46:08,691 INFO] Step 21000/40000; acc:  66.33; ppl:  3.80; xent: 1.33; lr: 0.00061; 17086/19095 tok/s;  18743 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 07:47:39,869 INFO] Step 21100/40000; acc:  65.58; ppl:  4.03; xent: 1.39; lr: 0.00061; 16413/18461 tok/s;  18834 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 07:49:09,992 INFO] Step 21200/40000; acc:  63.01; ppl:  4.46; xent: 1.49; lr: 0.00061; 17030/19073 tok/s;  18924 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 07:50:40,402 INFO] Step 21300/40000; acc:  63.44; ppl:  4.29; xent: 1.46; lr: 0.00061; 16947/19478 tok/s;  19015 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 07:52:10,816 INFO] Step 21400/40000; acc:  66.67; ppl:  3.72; xent: 1.31; lr: 0.00060; 17040/18418 tok/s;  19105 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 07:53:41,063 INFO] Step 21500/40000; acc:  63.33; ppl:  4.26; xent: 1.45; lr: 0.00060; 17140/18577 tok/s;  19195 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 07:55:04,447 INFO] Step 21600/40000; acc:  57.45; ppl:  5.53; xent: 1.71; lr: 0.00060; 17816/19705 tok/s;  19279 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 07:56:27,752 INFO] Step 21700/40000; acc:  60.61; ppl:  4.94; xent: 1.60; lr: 0.00060; 18094/19752 tok/s;  19362 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 07:57:51,844 INFO] Step 21800/40000; acc:  67.80; ppl:  3.66; xent: 1.30; lr: 0.00060; 18280/20518 tok/s;  19446 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 07:59:13,639 INFO] Step 21900/40000; acc:  65.10; ppl:  4.17; xent: 1.43; lr: 0.00060; 17697/20797 tok/s;  19528 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 08:00:37,151 INFO] Step 22000/40000; acc:  56.25; ppl:  5.79; xent: 1.76; lr: 0.00060; 18571/20769 tok/s;  19612 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 08:02:01,969 INFO] Step 22100/40000; acc:  61.61; ppl:  4.67; xent: 1.54; lr: 0.00059; 17159/19395 tok/s;  19696 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 08:03:32,949 INFO] Step 22200/40000; acc:  60.60; ppl:  4.99; xent: 1.61; lr: 0.00059; 17783/17660 tok/s;  19787 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 08:05:04,298 INFO] Step 22300/40000; acc:  53.77; ppl:  6.37; xent: 1.85; lr: 0.00059; 16135/17844 tok/s;  19879 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 08:06:35,749 INFO] Step 22400/40000; acc:  63.71; ppl:  4.34; xent: 1.47; lr: 0.00059; 16506/17910 tok/s;  19970 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 08:08:01,122 INFO] Step 22500/40000; acc:  58.54; ppl:  5.34; xent: 1.67; lr: 0.00059; 17998/19128 tok/s;  20056 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 08:09:25,955 INFO] Step 22600/40000; acc:  54.46; ppl:  6.32; xent: 1.84; lr: 0.00059; 18099/19625 tok/s;  20140 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 08:10:49,522 INFO] Step 22700/40000; acc:  60.29; ppl:  4.65; xent: 1.54; lr: 0.00059; 18267/21302 tok/s;  20224 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 08:12:16,131 INFO] Step 22800/40000; acc:  64.94; ppl:  4.17; xent: 1.43; lr: 0.00059; 17653/19314 tok/s;  20311 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 08:13:50,434 INFO] Step 22900/40000; acc:  60.90; ppl:  4.76; xent: 1.56; lr: 0.00058; 16416/18225 tok/s;  20405 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 08:15:22,924 INFO] Step 23000/40000; acc:  57.70; ppl:  5.45; xent: 1.69; lr: 0.00058; 16733/18707 tok/s;  20497 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 08:16:51,130 INFO] Step 23100/40000; acc:  72.26; ppl:  2.96; xent: 1.08; lr: 0.00058; 15022/18574 tok/s;  20586 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 08:18:22,200 INFO] Step 23200/40000; acc:  66.55; ppl:  3.72; xent: 1.31; lr: 0.00058; 17045/19130 tok/s;  20677 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 08:19:53,661 INFO] Step 23300/40000; acc:  61.50; ppl:  4.64; xent: 1.53; lr: 0.00058; 17119/18546 tok/s;  20768 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 08:21:26,799 INFO] Step 23400/40000; acc:  67.39; ppl:  3.57; xent: 1.27; lr: 0.00058; 18498/19373 tok/s;  20861 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 08:22:57,939 INFO] Step 23500/40000; acc:  57.19; ppl:  5.67; xent: 1.74; lr: 0.00058; 16402/18245 tok/s;  20952 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 08:24:27,007 INFO] Step 23600/40000; acc:  62.76; ppl:  4.49; xent: 1.50; lr: 0.00058; 17284/19073 tok/s;  21041 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 08:25:57,058 INFO] Step 23700/40000; acc:  59.71; ppl:  5.11; xent: 1.63; lr: 0.00057; 15767/18764 tok/s;  21131 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 08:27:24,061 INFO] Step 23800/40000; acc:  59.63; ppl:  5.18; xent: 1.64; lr: 0.00057; 17202/19211 tok/s;  21218 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 08:28:51,682 INFO] Step 23900/40000; acc:  54.96; ppl:  5.76; xent: 1.75; lr: 0.00057; 18782/19815 tok/s;  21306 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 08:30:19,706 INFO] Step 24000/40000; acc:  63.01; ppl:  3.84; xent: 1.35; lr: 0.00057; 21958/24458 tok/s;  21394 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 08:31:47,914 INFO] Step 24100/40000; acc:  67.15; ppl:  3.21; xent: 1.17; lr: 0.00057; 19735/24346 tok/s;  21482 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 08:33:19,896 INFO] Step 24200/40000; acc:  68.76; ppl:  3.05; xent: 1.12; lr: 0.00057; 19027/23710 tok/s;  21574 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 08:34:50,887 INFO] Step 24300/40000; acc:  57.95; ppl:  5.35; xent: 1.68; lr: 0.00057; 17199/18781 tok/s;  21665 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 08:36:24,458 INFO] Step 24400/40000; acc:  61.52; ppl:  4.66; xent: 1.54; lr: 0.00057; 16342/17841 tok/s;  21759 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 08:37:58,315 INFO] Step 24500/40000; acc:  54.87; ppl:  6.12; xent: 1.81; lr: 0.00056; 16301/18221 tok/s;  21853 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 08:39:29,170 INFO] Step 24600/40000; acc:  60.87; ppl:  4.64; xent: 1.53; lr: 0.00056; 16417/20189 tok/s;  21944 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 08:40:58,980 INFO] Step 24700/40000; acc:  66.22; ppl:  3.56; xent: 1.27; lr: 0.00056; 17548/23136 tok/s;  22033 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 08:42:32,706 INFO] Step 24800/40000; acc:  59.66; ppl:  5.01; xent: 1.61; lr: 0.00056; 16490/18837 tok/s;  22127 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 08:44:06,164 INFO] Step 24900/40000; acc:  57.62; ppl:  5.37; xent: 1.68; lr: 0.00056; 16807/18655 tok/s;  22221 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 08:45:40,294 INFO] Step 25000/40000; acc:  58.10; ppl:  5.22; xent: 1.65; lr: 0.00056; 17269/19072 tok/s;  22315 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 08:47:13,885 INFO] Step 25100/40000; acc:  58.67; ppl:  4.96; xent: 1.60; lr: 0.00056; 16433/18109 tok/s;  22408 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 08:48:46,815 INFO] Step 25200/40000; acc:  58.90; ppl:  4.98; xent: 1.61; lr: 0.00056; 16944/18630 tok/s;  22501 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 08:50:14,133 INFO] Step 25300/40000; acc:  56.75; ppl:  5.58; xent: 1.72; lr: 0.00056; 17387/19483 tok/s;  22589 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 08:51:38,556 INFO] Step 25400/40000; acc:  60.07; ppl:  4.98; xent: 1.61; lr: 0.00055; 18230/20330 tok/s;  22673 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 08:53:04,055 INFO] Step 25500/40000; acc:  57.45; ppl:  5.25; xent: 1.66; lr: 0.00055; 18178/19932 tok/s;  22758 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 08:54:32,244 INFO] Step 25600/40000; acc:  64.16; ppl:  4.16; xent: 1.43; lr: 0.00055; 19541/20897 tok/s;  22847 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 08:56:04,665 INFO] Step 25700/40000; acc:  65.54; ppl:  3.86; xent: 1.35; lr: 0.00055; 16743/18862 tok/s;  22939 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 08:57:37,107 INFO] Step 25800/40000; acc:  65.31; ppl:  3.83; xent: 1.34; lr: 0.00055; 17825/20132 tok/s;  23032 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 08:59:10,156 INFO] Step 25900/40000; acc:  56.78; ppl:  5.31; xent: 1.67; lr: 0.00055; 17047/19521 tok/s;  23125 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 09:00:46,471 INFO] Step 26000/40000; acc:  57.53; ppl:  5.38; xent: 1.68; lr: 0.00055; 16069/17705 tok/s;  23221 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 09:02:20,080 INFO] Step 26100/40000; acc:  59.61; ppl:  4.91; xent: 1.59; lr: 0.00055; 17031/19232 tok/s;  23314 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 09:03:52,323 INFO] Step 26200/40000; acc:  69.01; ppl:  3.38; xent: 1.22; lr: 0.00055; 16330/19374 tok/s;  23407 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 09:05:21,528 INFO] Step 26300/40000; acc:  82.65; ppl:  1.84; xent: 0.61; lr: 0.00055; 17913/22547 tok/s;  23496 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 09:06:53,781 INFO] Step 26400/40000; acc:  63.40; ppl:  4.17; xent: 1.43; lr: 0.00054; 16790/19532 tok/s;  23588 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 09:08:27,341 INFO] Step 26500/40000; acc:  60.07; ppl:  4.82; xent: 1.57; lr: 0.00054; 16584/18533 tok/s;  23682 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 09:10:02,648 INFO] Step 26600/40000; acc:  55.16; ppl:  5.90; xent: 1.77; lr: 0.00054; 16903/18168 tok/s;  23777 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 09:11:38,524 INFO] Step 26700/40000; acc:  56.73; ppl:  5.59; xent: 1.72; lr: 0.00054; 16106/17874 tok/s;  23873 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 09:13:13,199 INFO] Step 26800/40000; acc:  59.69; ppl:  4.90; xent: 1.59; lr: 0.00054; 16954/18298 tok/s;  23968 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 09:14:46,409 INFO] Step 26900/40000; acc:  60.47; ppl:  4.78; xent: 1.57; lr: 0.00054; 17145/18830 tok/s;  24061 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 09:16:19,496 INFO] Step 27000/40000; acc:  60.79; ppl:  4.79; xent: 1.57; lr: 0.00054; 17178/18375 tok/s;  24154 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 09:17:51,529 INFO] Step 27100/40000; acc:  62.05; ppl:  4.56; xent: 1.52; lr: 0.00054; 16539/18625 tok/s;  24246 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 09:19:25,411 INFO] Step 27200/40000; acc:  61.82; ppl:  4.53; xent: 1.51; lr: 0.00054; 16702/18133 tok/s;  24340 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 09:20:57,765 INFO] Step 27300/40000; acc:  59.99; ppl:  4.96; xent: 1.60; lr: 0.00053; 16189/18007 tok/s;  24432 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 09:22:31,701 INFO] Step 27400/40000; acc:  54.94; ppl:  5.90; xent: 1.77; lr: 0.00053; 15923/18453 tok/s;  24526 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 09:24:07,940 INFO] Step 27500/40000; acc:  60.84; ppl:  4.16; xent: 1.43; lr: 0.00053; 18925/20628 tok/s;  24622 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 09:25:40,764 INFO] Step 27600/40000; acc:  64.78; ppl:  3.62; xent: 1.29; lr: 0.00053; 18101/21886 tok/s;  24715 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 09:27:14,323 INFO] Step 27700/40000; acc:  63.06; ppl:  4.18; xent: 1.43; lr: 0.00053; 17080/19710 tok/s;  24809 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 09:28:45,252 INFO] Step 27800/40000; acc:  58.06; ppl:  5.32; xent: 1.67; lr: 0.00053; 16984/19673 tok/s;  24900 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 09:30:17,522 INFO] Step 27900/40000; acc:  59.72; ppl:  5.00; xent: 1.61; lr: 0.00053; 16400/18600 tok/s;  24992 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 09:31:49,045 INFO] Step 28000/40000; acc:  60.47; ppl:  4.67; xent: 1.54; lr: 0.00053; 16514/19200 tok/s;  25083 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 09:33:20,235 INFO] Step 28100/40000; acc:  59.14; ppl:  5.18; xent: 1.65; lr: 0.00053; 16699/18554 tok/s;  25175 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 09:34:53,514 INFO] Step 28200/40000; acc:  57.93; ppl:  5.47; xent: 1.70; lr: 0.00053; 15659/17661 tok/s;  25268 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 09:36:26,239 INFO] Step 28300/40000; acc:  60.03; ppl:  5.05; xent: 1.62; lr: 0.00053; 16255/18387 tok/s;  25361 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 09:38:04,294 INFO] Step 28400/40000; acc:  59.88; ppl:  4.91; xent: 1.59; lr: 0.00052; 16033/17539 tok/s;  25459 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 09:39:39,174 INFO] Step 28500/40000; acc:  61.49; ppl:  4.56; xent: 1.52; lr: 0.00052; 16403/18787 tok/s;  25554 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 09:41:12,527 INFO] Step 28600/40000; acc:  60.90; ppl:  4.72; xent: 1.55; lr: 0.00052; 17005/18936 tok/s;  25647 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 09:42:44,410 INFO] Step 28700/40000; acc:  60.68; ppl:  4.68; xent: 1.54; lr: 0.00052; 16687/19191 tok/s;  25739 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 09:44:23,310 INFO] Step 28800/40000; acc:  56.45; ppl:  5.91; xent: 1.78; lr: 0.00052; 16373/16260 tok/s;  25838 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 09:46:08,175 INFO] Step 28900/40000; acc:  56.64; ppl:  5.35; xent: 1.68; lr: 0.00052; 19776/20705 tok/s;  25943 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 09:47:54,419 INFO] Step 29000/40000; acc:  59.66; ppl:  4.56; xent: 1.52; lr: 0.00052; 21066/22450 tok/s;  26049 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 09:49:36,686 INFO] Step 29100/40000; acc:  63.10; ppl:  3.91; xent: 1.36; lr: 0.00052; 20127/23197 tok/s;  26151 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 09:51:14,448 INFO] Step 29200/40000; acc:  63.79; ppl:  3.78; xent: 1.33; lr: 0.00052; 21021/23059 tok/s;  26249 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 09:52:40,549 INFO] Step 29300/40000; acc:  64.44; ppl:  3.69; xent: 1.31; lr: 0.00052; 22853/25148 tok/s;  26335 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 09:54:05,520 INFO] Step 29400/40000; acc:  62.84; ppl:  4.31; xent: 1.46; lr: 0.00052; 19588/21933 tok/s;  26420 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 09:55:29,844 INFO] Step 29500/40000; acc:  61.73; ppl:  4.66; xent: 1.54; lr: 0.00051; 18398/20618 tok/s;  26504 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 09:56:55,613 INFO] Step 29600/40000; acc:  52.15; ppl:  7.02; xent: 1.95; lr: 0.00051; 18404/20277 tok/s;  26590 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 09:58:22,992 INFO] Step 29700/40000; acc:  58.83; ppl:  4.98; xent: 1.61; lr: 0.00051; 17834/19534 tok/s;  26677 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 09:59:51,319 INFO] Step 29800/40000; acc:  58.48; ppl:  5.28; xent: 1.66; lr: 0.00051; 17340/19172 tok/s;  26766 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 10:01:23,677 INFO] Step 29900/40000; acc:  59.92; ppl:  4.93; xent: 1.60; lr: 0.00051; 16966/18281 tok/s;  26858 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 10:03:00,034 INFO] Step 30000/40000; acc:  59.13; ppl:  5.11; xent: 1.63; lr: 0.00051; 16293/17582 tok/s;  26954 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 10:03:00,138 WARNING] The batch will be filled until we reach 1,its size may exceed 32 tokens\u001b[0m\n",
      "\u001b[34m[2022-11-26 10:03:00,139 WARNING] The batch will be filled until we reach 1,its size may exceed 32 tokens\u001b[0m\n",
      "\u001b[34m[2022-11-26 10:03:00,143 WARNING] The batch will be filled until we reach 1,its size may exceed 32 tokens\u001b[0m\n",
      "\u001b[34m[2022-11-26 10:03:20,791 WARNING] The batch will be filled until we reach 1,its size may exceed 32 tokens\u001b[0m\n",
      "\u001b[34m[2022-11-26 10:03:20,792 WARNING] The batch will be filled until we reach 1,its size may exceed 32 tokens\u001b[0m\n",
      "\u001b[34m[2022-11-26 10:03:20,793 WARNING] The batch will be filled until we reach 1,its size may exceed 32 tokens\u001b[0m\n",
      "\u001b[34m[2022-11-26 10:03:20,794 WARNING] The batch will be filled until we reach 1,its size may exceed 32 tokens\u001b[0m\n",
      "\u001b[34m[2022-11-26 10:03:20,795 WARNING] The batch will be filled until we reach 1,its size may exceed 32 tokens\u001b[0m\n",
      "\u001b[34m[2022-11-26 10:03:51,140 INFO] Validation perplexity: 6.85165\u001b[0m\n",
      "\u001b[34m[2022-11-26 10:03:51,140 INFO] Validation accuracy: 58.3425\u001b[0m\n",
      "\u001b[34m[2022-11-26 10:03:51,218 INFO] Saving checkpoint /opt/ml/model/model.en-ja_step_30000.pt\u001b[0m\n",
      "\u001b[34m[2022-11-26 10:05:24,952 INFO] Step 30100/40000; acc:  64.76; ppl:  4.06; xent: 1.40; lr: 0.00051; 10949/12019 tok/s;  27099 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 10:06:55,613 INFO] Step 30200/40000; acc:  62.31; ppl:  4.43; xent: 1.49; lr: 0.00051; 16482/19217 tok/s;  27190 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 10:08:30,812 INFO] Step 30300/40000; acc:  78.56; ppl:  2.30; xent: 0.83; lr: 0.00051; 19563/20017 tok/s;  27285 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 10:10:00,139 INFO] Step 30400/40000; acc:  91.04; ppl:  1.40; xent: 0.34; lr: 0.00051; 27452/26381 tok/s;  27375 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 10:11:26,070 INFO] Step 30500/40000; acc:  62.92; ppl:  4.36; xent: 1.47; lr: 0.00051; 19101/20667 tok/s;  27460 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 10:12:49,974 INFO] Step 30600/40000; acc:  63.43; ppl:  4.30; xent: 1.46; lr: 0.00051; 18895/21062 tok/s;  27544 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 10:14:14,266 INFO] Step 30700/40000; acc:  60.81; ppl:  4.88; xent: 1.58; lr: 0.00050; 18228/20423 tok/s;  27629 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 10:15:36,880 INFO] Step 30800/40000; acc:  61.79; ppl:  4.59; xent: 1.52; lr: 0.00050; 18485/20302 tok/s;  27711 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 10:17:06,698 INFO] Step 30900/40000; acc:  63.34; ppl:  4.61; xent: 1.53; lr: 0.00050; 18504/22543 tok/s;  27801 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 10:18:42,672 INFO] Step 31000/40000; acc:  74.06; ppl:  3.00; xent: 1.10; lr: 0.00050; 19061/26208 tok/s;  27897 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 10:20:17,695 INFO] Step 31100/40000; acc:  74.86; ppl:  2.93; xent: 1.08; lr: 0.00050; 17664/26363 tok/s;  27992 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 10:21:50,935 INFO] Step 31200/40000; acc:  65.83; ppl:  4.13; xent: 1.42; lr: 0.00050; 16166/21057 tok/s;  28085 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 10:23:24,579 INFO] Step 31300/40000; acc:  60.17; ppl:  4.83; xent: 1.57; lr: 0.00050; 16465/18679 tok/s;  28179 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 10:24:56,460 INFO] Step 31400/40000; acc:  61.62; ppl:  4.64; xent: 1.53; lr: 0.00050; 16577/18653 tok/s;  28271 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 10:26:28,593 INFO] Step 31500/40000; acc:  57.47; ppl:  5.23; xent: 1.65; lr: 0.00050; 17048/19446 tok/s;  28363 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 10:27:59,365 INFO] Step 31600/40000; acc:  64.28; ppl:  3.99; xent: 1.38; lr: 0.00050; 16913/19211 tok/s;  28454 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 10:29:32,367 INFO] Step 31700/40000; acc:  65.61; ppl:  3.74; xent: 1.32; lr: 0.00050; 16097/18467 tok/s;  28547 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 10:31:05,327 INFO] Step 31800/40000; acc:  67.64; ppl:  3.40; xent: 1.22; lr: 0.00050; 17806/19446 tok/s;  28640 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 10:32:39,506 INFO] Step 31900/40000; acc:  59.54; ppl:  4.76; xent: 1.56; lr: 0.00049; 17345/18350 tok/s;  28734 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 10:34:12,477 INFO] Step 32000/40000; acc:  83.59; ppl:  1.95; xent: 0.67; lr: 0.00049; 23178/23005 tok/s;  28827 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 10:35:49,514 INFO] Step 32100/40000; acc:  95.75; ppl:  1.22; xent: 0.20; lr: 0.00049; 24080/25983 tok/s;  28924 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 10:37:24,062 INFO] Step 32200/40000; acc:  90.43; ppl:  1.51; xent: 0.41; lr: 0.00049; 24847/23875 tok/s;  29018 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 10:38:58,033 INFO] Step 32300/40000; acc:  67.74; ppl:  3.62; xent: 1.29; lr: 0.00049; 17509/19300 tok/s;  29112 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 10:40:31,282 INFO] Step 32400/40000; acc:  62.38; ppl:  4.46; xent: 1.49; lr: 0.00049; 17344/19047 tok/s;  29206 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 10:41:57,755 INFO] Step 32500/40000; acc:  63.10; ppl:  4.34; xent: 1.47; lr: 0.00049; 17640/19450 tok/s;  29292 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 10:43:23,146 INFO] Step 32600/40000; acc:  57.94; ppl:  5.14; xent: 1.64; lr: 0.00049; 18496/19804 tok/s;  29378 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 10:44:50,368 INFO] Step 32700/40000; acc:  58.18; ppl:  5.04; xent: 1.62; lr: 0.00049; 18882/20581 tok/s;  29465 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 10:46:18,956 INFO] Step 32800/40000; acc:  60.20; ppl:  4.94; xent: 1.60; lr: 0.00049; 17253/19326 tok/s;  29553 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 10:47:52,821 INFO] Step 32900/40000; acc:  59.75; ppl:  5.05; xent: 1.62; lr: 0.00049; 16612/17730 tok/s;  29647 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 10:49:22,954 INFO] Step 33000/40000; acc:  59.76; ppl:  4.93; xent: 1.60; lr: 0.00049; 17330/19015 tok/s;  29737 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 10:50:48,286 INFO] Step 33100/40000; acc:  59.15; ppl:  5.02; xent: 1.61; lr: 0.00049; 18725/20091 tok/s;  29823 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 10:52:10,714 INFO] Step 33200/40000; acc:  53.30; ppl:  6.33; xent: 1.85; lr: 0.00049; 19464/19636 tok/s;  29905 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 10:53:34,984 INFO] Step 33300/40000; acc:  57.09; ppl:  5.52; xent: 1.71; lr: 0.00048; 18197/19684 tok/s;  29989 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 10:55:02,772 INFO] Step 33400/40000; acc:  62.72; ppl:  4.40; xent: 1.48; lr: 0.00048; 18345/19539 tok/s;  30077 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 10:56:30,798 INFO] Step 33500/40000; acc:  57.80; ppl:  5.08; xent: 1.63; lr: 0.00048; 18305/20111 tok/s;  30165 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 10:57:55,734 INFO] Step 33600/40000; acc:  57.63; ppl:  5.13; xent: 1.64; lr: 0.00048; 19824/21688 tok/s;  30250 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 10:59:21,129 INFO] Step 33700/40000; acc:  55.09; ppl:  6.02; xent: 1.80; lr: 0.00048; 17622/19792 tok/s;  30336 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 11:00:48,289 INFO] Step 33800/40000; acc:  57.93; ppl:  5.41; xent: 1.69; lr: 0.00048; 17710/18916 tok/s;  30423 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 11:02:19,813 INFO] Step 33900/40000; acc:  61.25; ppl:  4.61; xent: 1.53; lr: 0.00048; 17191/18683 tok/s;  30514 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 11:03:56,529 INFO] Step 34000/40000; acc:  62.78; ppl:  4.29; xent: 1.46; lr: 0.00048; 16346/18081 tok/s;  30611 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 11:05:30,080 INFO] Step 34100/40000; acc:  61.26; ppl:  4.61; xent: 1.53; lr: 0.00048; 16696/18256 tok/s;  30704 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 11:07:03,596 INFO] Step 34200/40000; acc:  56.90; ppl:  5.43; xent: 1.69; lr: 0.00048; 16742/17931 tok/s;  30798 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 11:08:35,706 INFO] Step 34300/40000; acc:  61.09; ppl:  4.70; xent: 1.55; lr: 0.00048; 16679/17970 tok/s;  30890 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 11:10:09,146 INFO] Step 34400/40000; acc:  73.39; ppl:  2.88; xent: 1.06; lr: 0.00048; 18084/19469 tok/s;  30984 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 11:11:49,691 INFO] Step 34500/40000; acc:  87.02; ppl:  1.69; xent: 0.52; lr: 0.00048; 21202/21233 tok/s;  31084 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 11:13:35,128 INFO] Step 34600/40000; acc:  99.02; ppl:  1.07; xent: 0.07; lr: 0.00048; 24490/26412 tok/s;  31190 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 11:15:18,605 INFO] Step 34700/40000; acc:  98.69; ppl:  1.08; xent: 0.08; lr: 0.00047; 22913/27051 tok/s;  31293 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 11:16:55,989 INFO] Step 34800/40000; acc:  72.49; ppl:  3.02; xent: 1.10; lr: 0.00047; 17870/19542 tok/s;  31390 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 11:18:31,427 INFO] Step 34900/40000; acc:  56.50; ppl:  5.69; xent: 1.74; lr: 0.00047; 15591/17823 tok/s;  31486 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 11:20:05,491 INFO] Step 35000/40000; acc:  55.08; ppl:  5.93; xent: 1.78; lr: 0.00047; 16349/18094 tok/s;  31580 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 11:21:37,579 INFO] Step 35100/40000; acc:  54.82; ppl:  6.14; xent: 1.81; lr: 0.00047; 15133/17449 tok/s;  31672 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 11:23:13,007 INFO] Step 35200/40000; acc:  57.62; ppl:  5.44; xent: 1.69; lr: 0.00047; 16062/17530 tok/s;  31767 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 11:24:47,907 INFO] Step 35300/40000; acc:  55.08; ppl:  6.15; xent: 1.82; lr: 0.00047; 15468/17943 tok/s;  31862 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 11:26:20,448 INFO] Step 35400/40000; acc:  59.67; ppl:  4.80; xent: 1.57; lr: 0.00047; 16704/19875 tok/s;  31955 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 11:27:55,123 INFO] Step 35500/40000; acc:  60.90; ppl:  4.55; xent: 1.52; lr: 0.00047; 17560/18770 tok/s;  32050 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 11:29:26,728 INFO] Step 35600/40000; acc:  55.42; ppl:  6.04; xent: 1.80; lr: 0.00047; 16537/18688 tok/s;  32141 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 11:30:55,219 INFO] Step 35700/40000; acc:  60.21; ppl:  4.81; xent: 1.57; lr: 0.00047; 16290/18531 tok/s;  32230 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 11:32:21,894 INFO] Step 35800/40000; acc:  57.96; ppl:  5.29; xent: 1.67; lr: 0.00047; 18740/20255 tok/s;  32316 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 11:33:50,434 INFO] Step 35900/40000; acc:  60.11; ppl:  4.70; xent: 1.55; lr: 0.00047; 16111/18903 tok/s;  32405 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 11:35:24,915 INFO] Step 36000/40000; acc:  58.78; ppl:  5.28; xent: 1.66; lr: 0.00047; 14681/17278 tok/s;  32499 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 11:36:58,637 INFO] Step 36100/40000; acc:  57.43; ppl:  5.30; xent: 1.67; lr: 0.00047; 16268/18220 tok/s;  32593 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 11:38:29,270 INFO] Step 36200/40000; acc:  66.07; ppl:  3.65; xent: 1.29; lr: 0.00046; 17004/19740 tok/s;  32684 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 11:40:03,634 INFO] Step 36300/40000; acc:  68.73; ppl:  3.38; xent: 1.22; lr: 0.00046; 17478/19199 tok/s;  32778 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 11:41:38,926 INFO] Step 36400/40000; acc:  58.10; ppl:  5.10; xent: 1.63; lr: 0.00046; 17024/18621 tok/s;  32873 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 11:43:12,509 INFO] Step 36500/40000; acc:  56.59; ppl:  5.53; xent: 1.71; lr: 0.00046; 16874/18375 tok/s;  32967 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 11:44:50,489 INFO] Step 36600/40000; acc:  58.73; ppl:  5.25; xent: 1.66; lr: 0.00046; 16682/17187 tok/s;  33065 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 11:46:26,313 INFO] Step 36700/40000; acc:  53.01; ppl:  6.43; xent: 1.86; lr: 0.00046; 17274/17814 tok/s;  33161 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 11:47:57,180 INFO] Step 36800/40000; acc:  55.92; ppl:  5.33; xent: 1.67; lr: 0.00046; 18919/20340 tok/s;  33252 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 11:49:21,889 INFO] Step 36900/40000; acc:  56.30; ppl:  5.69; xent: 1.74; lr: 0.00046; 18078/19402 tok/s;  33336 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 11:50:49,546 INFO] Step 37000/40000; acc:  59.33; ppl:  4.94; xent: 1.60; lr: 0.00046; 18550/19454 tok/s;  33424 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 11:52:16,465 INFO] Step 37100/40000; acc:  56.12; ppl:  5.63; xent: 1.73; lr: 0.00046; 17807/19402 tok/s;  33511 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 11:53:42,527 INFO] Step 37200/40000; acc:  59.11; ppl:  4.84; xent: 1.58; lr: 0.00046; 18319/19721 tok/s;  33597 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 11:55:07,124 INFO] Step 37300/40000; acc:  56.24; ppl:  5.61; xent: 1.72; lr: 0.00046; 18661/20275 tok/s;  33682 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 11:56:42,423 INFO] Step 37400/40000; acc:  74.30; ppl:  2.88; xent: 1.06; lr: 0.00046; 22022/21191 tok/s;  33777 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 11:58:12,401 INFO] Step 37500/40000; acc:  61.20; ppl:  4.52; xent: 1.51; lr: 0.00046; 17867/19870 tok/s;  33867 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 11:59:37,647 INFO] Step 37600/40000; acc:  56.75; ppl:  5.57; xent: 1.72; lr: 0.00046; 16300/18377 tok/s;  33952 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 12:01:02,998 INFO] Step 37700/40000; acc:  57.76; ppl:  5.28; xent: 1.66; lr: 0.00046; 17404/19326 tok/s;  34037 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 12:02:32,351 INFO] Step 37800/40000; acc:  58.45; ppl:  5.18; xent: 1.65; lr: 0.00045; 16777/18278 tok/s;  34127 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 12:04:06,664 INFO] Step 37900/40000; acc:  58.13; ppl:  5.25; xent: 1.66; lr: 0.00045; 15509/17110 tok/s;  34221 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 12:05:40,809 INFO] Step 38000/40000; acc:  58.45; ppl:  5.17; xent: 1.64; lr: 0.00045; 16443/17890 tok/s;  34315 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 12:07:12,783 INFO] Step 38100/40000; acc:  55.70; ppl:  5.91; xent: 1.78; lr: 0.00045; 16284/18318 tok/s;  34407 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 12:08:47,675 INFO] Step 38200/40000; acc:  56.50; ppl:  5.45; xent: 1.70; lr: 0.00045; 15924/17413 tok/s;  34502 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 12:10:21,029 INFO] Step 38300/40000; acc:  57.61; ppl:  5.26; xent: 1.66; lr: 0.00045; 16513/17708 tok/s;  34595 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 12:11:53,026 INFO] Step 38400/40000; acc:  56.71; ppl:  5.68; xent: 1.74; lr: 0.00045; 16569/17912 tok/s;  34687 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 12:13:27,465 INFO] Step 38500/40000; acc:  55.16; ppl:  5.79; xent: 1.76; lr: 0.00045; 15591/17114 tok/s;  34782 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 12:15:04,277 INFO] Step 38600/40000; acc:  56.47; ppl:  5.62; xent: 1.73; lr: 0.00045; 16090/17061 tok/s;  34879 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 12:16:40,602 INFO] Step 38700/40000; acc:  57.90; ppl:  5.36; xent: 1.68; lr: 0.00045; 16923/17749 tok/s;  34975 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 12:18:13,862 INFO] Step 38800/40000; acc:  58.87; ppl:  5.09; xent: 1.63; lr: 0.00045; 16131/17765 tok/s;  35068 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 12:19:47,999 INFO] Step 38900/40000; acc:  58.39; ppl:  5.21; xent: 1.65; lr: 0.00045; 16256/18151 tok/s;  35162 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 12:21:22,641 INFO] Step 39000/40000; acc:  62.14; ppl:  4.42; xent: 1.49; lr: 0.00045; 16597/18645 tok/s;  35257 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 12:22:56,990 INFO] Step 39100/40000; acc:  59.61; ppl:  5.00; xent: 1.61; lr: 0.00045; 17785/18755 tok/s;  35351 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 12:24:28,375 INFO] Step 39200/40000; acc:  68.64; ppl:  3.42; xent: 1.23; lr: 0.00045; 16665/19272 tok/s;  35443 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 12:26:01,676 INFO] Step 39300/40000; acc:  62.57; ppl:  4.29; xent: 1.46; lr: 0.00045; 16685/18432 tok/s;  35536 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 12:27:36,729 INFO] Step 39400/40000; acc:  63.52; ppl:  4.12; xent: 1.42; lr: 0.00045; 17336/18859 tok/s;  35631 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 12:29:09,397 INFO] Step 39500/40000; acc:  67.72; ppl:  3.35; xent: 1.21; lr: 0.00044; 18544/21046 tok/s;  35724 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 12:30:43,518 INFO] Step 39600/40000; acc:  58.18; ppl:  5.12; xent: 1.63; lr: 0.00044; 16011/17291 tok/s;  35818 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 12:32:18,352 INFO] Step 39700/40000; acc:  57.03; ppl:  5.51; xent: 1.71; lr: 0.00044; 17077/18104 tok/s;  35913 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 12:33:49,007 INFO] Step 39800/40000; acc:  65.56; ppl:  3.76; xent: 1.32; lr: 0.00044; 19107/23173 tok/s;  36003 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 12:35:23,113 INFO] Step 39900/40000; acc:  60.56; ppl:  4.64; xent: 1.53; lr: 0.00044; 17153/18742 tok/s;  36098 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 12:36:59,199 INFO] Step 40000/40000; acc:  59.63; ppl:  4.81; xent: 1.57; lr: 0.00044; 16795/17918 tok/s;  36194 sec;\u001b[0m\n",
      "\u001b[34m[2022-11-26 12:36:59,266 WARNING] The batch will be filled until we reach 1,its size may exceed 32 tokens\u001b[0m\n",
      "\u001b[34m[2022-11-26 12:36:59,266 WARNING] The batch will be filled until we reach 1,its size may exceed 32 tokens\u001b[0m\n",
      "\u001b[34m[2022-11-26 12:36:59,270 WARNING] The batch will be filled until we reach 1,its size may exceed 32 tokens\u001b[0m\n",
      "\u001b[34m[2022-11-26 12:37:19,961 WARNING] The batch will be filled until we reach 1,its size may exceed 32 tokens\u001b[0m\n",
      "\u001b[34m[2022-11-26 12:37:19,962 WARNING] The batch will be filled until we reach 1,its size may exceed 32 tokens\u001b[0m\n",
      "\u001b[34m[2022-11-26 12:37:19,963 WARNING] The batch will be filled until we reach 1,its size may exceed 32 tokens\u001b[0m\n",
      "\u001b[34m[2022-11-26 12:37:19,964 WARNING] The batch will be filled until we reach 1,its size may exceed 32 tokens\u001b[0m\n",
      "\u001b[34m[2022-11-26 12:37:19,965 WARNING] The batch will be filled until we reach 1,its size may exceed 32 tokens\u001b[0m\n",
      "\u001b[34m[2022-11-26 12:37:50,429 INFO] Validation perplexity: 6.20128\u001b[0m\n",
      "\u001b[34m[2022-11-26 12:37:50,429 INFO] Validation accuracy: 60.1576\u001b[0m\n",
      "\u001b[34m[2022-11-26 12:37:50,504 INFO] Saving checkpoint /opt/ml/model/model.en-ja_step_40000.pt\u001b[0m\n",
      "\n",
      "2022-11-26 12:38:47 Uploading - Uploading generated training model\n",
      "2022-11-26 12:43:51 Completed - Training job completed\n",
      "Training seconds: 36763\n",
      "Billable seconds: 36763\n"
     ]
    }
   ],
   "source": [
    "account = sess.boto_session.client('sts').get_caller_identity()['Account']\n",
    "region = sess.boto_session.region_name\n",
    "image = '{}.dkr.ecr.{}.amazonaws.com/jparacrawl-train:latest'.format(account, region)\n",
    "\n",
    "estimator = sage.estimator.Estimator(image,\n",
    "                        role, 1, 'ml.g4dn.12xlarge',\n",
    "                        output_path=\"s3://{}/jparacrawl/output\".format(sess.default_bucket()),\n",
    "                        sagemaker_session=sess)\n",
    "\n",
    "estimator.fit({\"training\": data_location})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5ca4f9f-9b4d-4587-9871-bf52b9c6d718",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir data_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65e25831-c9e8-4553-924b-fdc2281d0356",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'jparacrawl/output/jparacrawl-train-2022-11-26-02-29-37-291/output/model.tar.gz'\n",
    "MODEL_DIRECTORY = 'data_model'\n",
    "sess.download_data(MODEL_DIRECTORY, sess.default_bucket(), key_prefix=prefix)\n",
    "\n",
    "# sess.download_data(MODEL_DIRECTORY, sess.default_bucket(), key_prefix=estimator.model_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "368b390d-09f6-4861-b7ba-6c20e01d7f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/tetsuya/repo/programming-study/aws_sagemaker/sagemaker_jparacrawl_mecab/data_model\n"
     ]
    }
   ],
   "source": [
    "%cd data_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6400862-4620-4492-a0ee-1dabc7c5f329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.en-ja_step_40000.pt\n",
      "model.en-ja_step_10000.pt\n",
      "model.en-ja_step_20000.pt\n",
      "model.en-ja_step_30000.pt\n"
     ]
    }
   ],
   "source": [
    "!tar xvf model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8577377-b82f-4c46-a50e-8d6d1076e88a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/tetsuya/repo/programming-study/aws_sagemaker/sagemaker_jparacrawl_mecab\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "700c3f93-fa2c-44cb-a121-464ec78da91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp data/en-ja/sentencepiece_en.model data_serve/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d0d5889-b955-4403-8326-1f9f334dda95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/tetsuya/repo/programming-study/aws_sagemaker/sagemaker_jparacrawl_mecab/local_translate\n"
     ]
    }
   ],
   "source": [
    "%cd local_translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "75ef6364-1df5-4ebc-8e4e-cd9a6e5a7768",
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod +x src/translate.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f646d8c8-4744-42a1-a4cd-c65b12bc5060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  6.656kB\n",
      "Step 1/8 : FROM ubuntu:22.04\n",
      " ---> 2dc39ba059dc\n",
      "Step 2/8 : RUN apt-get -y update\n",
      " ---> Using cache\n",
      " ---> c7133dee31b8\n",
      "Step 3/8 : RUN apt-get -y install python3 pip\n",
      " ---> Using cache\n",
      " ---> ec402a346ff5\n",
      "Step 4/8 : RUN pip install ctranslate2 OpenNMT-py sentencepiece\n",
      " ---> Using cache\n",
      " ---> 0dacd89ab0b6\n",
      "Step 5/8 : COPY src/translate.py /usr/bin/\n",
      " ---> 87cbfdbae61e\n",
      "Step 6/8 : WORKDIR /var/opt/ctranslate2\n",
      " ---> Running in 51917df973d1\n",
      "Removing intermediate container 51917df973d1\n",
      " ---> b0c1fda78bd1\n",
      "Step 7/8 : ENTRYPOINT [ \"translate.py\" ]\n",
      " ---> Running in 8913963c315c\n",
      "Removing intermediate container 8913963c315c\n",
      " ---> e479d7710ca7\n",
      "Step 8/8 : CMD [ \"Hello world!\" ]\n",
      " ---> Running in 5b59a8aef6dc\n",
      "Removing intermediate container 5b59a8aef6dc\n",
      " ---> bcff91a468f8\n",
      "Successfully built bcff91a468f8\n",
      "Successfully tagged ctarnslate2_jpara_mecab:latest\n"
     ]
    }
   ],
   "source": [
    "!docker build -t ctarnslate2_jpara_mecab:latest ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dff65b48-67f4-4c50-a849-e1f758711152",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd ../data_serve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6ea17bb2-1664-4969-be47-8c61ff061493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/translate.py\", line 7, in <module>\n",
      "    translator = ctranslate2.Translator(\"ctranslate2_model\", device=\"cpu\")\n",
      "RuntimeError: Unable to open file 'model.bin' in model 'ctranslate2_model'\n"
     ]
    }
   ],
   "source": [
    "!docker run --rm -v $PWD:/var/opt/ctranslate2 ctarnslate2_jpara_mecab:latest 'Hello world'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
